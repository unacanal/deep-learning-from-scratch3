{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 37 텐서를 다루다\n",
    "지금까지 구현한 함수들이 텐서도 잘 다룬다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[ 0.84147098  0.90929743  0.14112001]\n",
      "          [-0.7568025  -0.95892427 -0.2794155 ]])\n",
      "variable([[11 22 33]\n",
      "          [44 55 66]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dezero.functions as F\n",
    "from dezero import Variable\n",
    "\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.sin(x)\n",
    "print(y)\n",
    "\n",
    "c = Variable(np.array([[10, 20, 30], [40, 50, 60]]))\n",
    "print(x + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(1)\n",
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n",
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n",
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "t = x + c\n",
    "y = F.sum(t)\n",
    "\n",
    "y.backward(retain_grad=True)\n",
    "print(y.grad)\n",
    "print(t.grad)\n",
    "print(x.grad)\n",
    "print(c.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기울기의 형상과 데이터의 형상이 일치한다. (`x.shape == x.grad.shape`)\n",
    "- 텐서의 미분을 머신러닝에서는 기울기(gradient)라고 한다.\n",
    "\n",
    "#### 역전파 고찰\n",
    "- $\\bold{x,y}$ 벡터의 원소가 n개일 때, $\\bold{y}=F(\\bold{x})$의 미분 결과는 nxn 행렬이 된다. (야코비 행렬 Jacobian matrix)\n",
    "- 자동 미분의 forward 모드를 수행하면 행렬 곱의 결과가 다시 행렬이 되는 것을 반복한다. (nxn 행렬을 전파한다)\n",
    "- reverse 모드는 출력이 스칼라이므로 n개의 벡터를 전파한다.\n",
    "- 그래서 역전파가 계산 효율이 더 좋다.\n",
    "\n",
    "- 결과만 필요한 상황이라면 명시적으로 야코비 행렬을 구해서 행렬의 곱을 계산할 필요가 없다.\n",
    "- 원소별 연산의 야코비 행렬은 대각 행렬이 된다. 최종 결과는 원소별 미분을 계산한 후 그 결과값을 원소별로 곱하면 얻을 수 있다.\n",
    "- 원소별 연산에서는 역전파도 미분을 원소별로 곱하여 구한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 38 형상 변환 함수\n",
    "reshape, transpose\n",
    "\n",
    "#### Reshape 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.reshape(x, (6,))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import Function\n",
    "\n",
    "class Reshape(Function):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = x.reshape(self.shape)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        return reshape(gy, self.x_shape)\n",
    "\n",
    "\n",
    "from dezero.core import as_variable\n",
    "def reshape(x, shape):\n",
    "    if x.shape == shape:\n",
    "        return as_variable(x)\n",
    "    return Reshape(shape)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.reshape(x, (6,))\n",
    "y.backward(retain_grad=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero\n",
    "class Variable:\n",
    "    # ...\n",
    "    def reshape(self, *shape):\n",
    "        if len(shape) == 1 and isinstance(shape[0], (tuple, list)):\n",
    "            shape = shape[0]\n",
    "\n",
    "        # F.reshape가 아니라 아래처럼 쓴 이유: 순환 임포트를 피하기 위해서\n",
    "        return dezero.functions.reshape(self, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[-2.06150881  0.02763206  0.63438151]\n",
      "          [ 0.88700969 -1.05686728  0.89565575]])\n",
      "variable([[-2.06150881  0.02763206  0.63438151]\n",
      "          [ 0.88700969 -1.05686728  0.89565575]])\n"
     ]
    }
   ],
   "source": [
    "from dezero import Variable\n",
    "x = Variable(np.random.randn(1, 2, 3))\n",
    "y = x.reshape((2, 3))\n",
    "print(y)\n",
    "y = x.reshape(2, 3)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(Function):\n",
    "    def forward(self, x):\n",
    "        y = np.transpose(x)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        gx = transpose(gy)\n",
    "        return gx\n",
    "\n",
    "def transpose(x):\n",
    "    return Transpose()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.transpose(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[-0.17898792 -0.7562412  -0.09146235]\n",
      "          [ 0.53685879  0.14452229  1.44430452]])\n",
      "variable([[-0.17898792  0.53685879]\n",
      "          [-0.7562412   0.14452229]\n",
      "          [-0.09146235  1.44430452]])\n"
     ]
    }
   ],
   "source": [
    "class Variable:\n",
    "    # ...\n",
    "    def transpose(self):\n",
    "        # F.reshape가 아니라 아래처럼 쓴 이유: 순환 임포트를 피하기 위해서\n",
    "        return dezero.functions.transpose(self)\n",
    "    \n",
    "    # 인스턴스 변수로 사용할 수 있도록!\n",
    "    @property\n",
    "    def T(self):\n",
    "        return dezero.functions.transpose(self)\n",
    "\n",
    "from dezero import Variable\n",
    "x = Variable(np.random.randn(2, 3))\n",
    "print(x)\n",
    "y = x.transpose()\n",
    "y = x.T\n",
    "print(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 39 합계 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.functions import broadcast_to\n",
    "class Sum(Function):\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = x.sum()\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = broadcast_to(gy, self.x_shape)\n",
    "        return gx\n",
    "    \n",
    "def sum(x):\n",
    "    return Sum()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(21)\n",
      "variable([1 1 1 1 1 1])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([1, 2, 3, 4, 5, 6]))\n",
    "y = F.sum(x)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 9]\n",
      "(2, 3)  ->  (3,)\n"
     ]
    }
   ],
   "source": [
    "# np.sum()\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.sum(x, axis=0)\n",
    "print(y)\n",
    "print(x.shape, ' -> ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21]]\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "# np.sum()\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.sum(x, keepdims=True)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.functions import broadcast_to\n",
    "from dezero import utils\n",
    "\n",
    "class Sum(Function):\n",
    "    def __init__(self, axis, keepdims):\n",
    "        self.axis = axis\n",
    "        self.keepdims = keepdims\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = x.sum(axis=self.axis, keepdims=self.keepdims)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gy = utils.reshape_sum_backward(gy, self.x_shape, self.axis, self.keepdims)\n",
    "        gx = broadcast_to(gy, self.x_shape)\n",
    "        return gx\n",
    "    \n",
    "def sum(x, axis=None, keepdims=False):\n",
    "    return Sum(axis, keepdims)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    #...\n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        return dezero.functions.sum(self, axis, keepdims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([5 7 9])\n",
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n",
      "(1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# 사용 예\n",
    "from dezero import Variable\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "y = F.sum(x, axis=0)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)\n",
    "\n",
    "x = Variable(np.random.randn(2, 3, 4, 5))\n",
    "y = x.sum(keepdims=True)\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 40 브로드캐스트 함수\n",
    "`broadcast_to`, `sum_to` 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.broadcast_to(x, shape)\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.broadcast_to(x, (2, 3))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 7 9]]\n",
      "[[ 6]\n",
      " [15]]\n"
     ]
    }
   ],
   "source": [
    "# sum_to\n",
    "from dezero.utils import sum_to\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = sum_to(x, (1, 3))\n",
    "print(y)\n",
    "\n",
    "y = sum_to(x, (2, 1))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BroadcastTo(Function):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = np.broadcast_to(x, self.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = sum_to(gy, self.x_shape)\n",
    "        return gx\n",
    "\n",
    "def broadcast_to(x, shape):\n",
    "    if x.shape == shape:\n",
    "        return as_variable(x)\n",
    "    return BroadcastTo(shape)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTo(Function):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = utils.sum_to(x, self.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = broadcast_to(gy, self.x_shape)\n",
    "        return gx\n",
    "    \n",
    "def sum_to(x, shape):\n",
    "    if x.shape == shape:\n",
    "        return as_variable(x)\n",
    "    return SumTo(shape)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 클래스 수정\n",
    "class Add(Function):\n",
    "    def forward(self, x0, x1):\n",
    "        self.x0_shape, self.x1_shape = x0.shape, x1.shape #\n",
    "        y = x0 + x1\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx0, gx1 = gy, gy\n",
    "        if self.x0_shape != self.x1_shape: #\n",
    "            gx0 = dezero.functions.sum_to(gx0, self.x0_shape) #\n",
    "            gx1 = dezero.functions.sum_to(gx1, self.x1_shape) #\n",
    "        return gx0, gx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([11 12 13])\n",
      "variable([3])\n",
      "variable([1 1 1])\n"
     ]
    }
   ],
   "source": [
    "# 구현 확인\n",
    "x0 = Variable(np.array([1, 2, 3]))\n",
    "x1 = Variable(np.array([10]))\n",
    "y = x0 + x1\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x1.grad)\n",
    "print(x0.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 41 행렬의 곱\n",
    "\n",
    "- 벡터의 내적: 두 벡터 사이의 대응 원소의 곱을 모두 합한 값\n",
    "- 행렬의 곱: 왼쪽 행렬의 가로방향 벡터와 오른쪽 행렬의 세로방향 벡터 사이의 내적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "# 벡터의 내적\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "c = np.dot(a, b)\n",
    "print(c)\n",
    "\n",
    "# 행렬의 곱\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6], [7, 8]])\n",
    "c = np.dot(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul(Function):\n",
    "    def forward(self, x, W):\n",
    "        y = x.dot(W)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x, W = self.inputs\n",
    "        gx = matmul(gy, W.T)\n",
    "        gW = matmul(x.T, gy)\n",
    "        return gx, gW\n",
    "\n",
    "def matmul(x, W):\n",
    "    return MatMul()(x, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.random.randn(2, 3))\n",
    "W = Variable(np.random.randn(3, 4))\n",
    "y = F.matmul(x, W)\n",
    "y.backward()\n",
    "\n",
    "print(x.grad.shape)\n",
    "print(W.grad.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 42 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2084bacdb20>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.randn(100, 1)\n",
    "y = 5 + 2 * x + np.random.randn(100, 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "W = Variable(np.zeros((1, 1)))\n",
    "b = Variable(np.zeros(1))\n",
    "\n",
    "def predict(x):\n",
    "    y = F.matmul(x, W) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[0.49185321]]) variable([1.0403258]) variable(32.65684143290435)\n",
      "variable([[0.87098307]]) variable([1.86670309]) variable(20.8049653938767)\n",
      "variable([[1.16293087]]) variable([2.52326992]) variable(13.405480828635747)\n",
      "variable([[1.38750254]]) variable([3.04503122]) variable(8.783241422956205)\n",
      "variable([[1.56004728]]) variable([3.45975402]) variable(5.894230127462607)\n",
      "variable([[1.69245272]]) variable([3.78946835]) variable(4.087467116592623)\n",
      "variable([[1.79391933]]) variable([4.05165603]) variable(2.9568445746276204)\n",
      "variable([[1.87156267]]) variable([4.26019247]) variable(2.248885834520452)\n",
      "variable([[1.9308816]]) variable([4.42609289]) variable(1.8052961399692495)\n",
      "variable([[1.97612211]]) variable([4.55810367]) variable(1.5271664114616335)\n",
      "variable([[2.01055989]]) variable([4.66317115]) variable(1.352658545853154)\n",
      "variable([[2.0367197]]) variable([4.7468132]) variable(1.2430881436096468)\n",
      "variable([[2.05654553]]) variable([4.81341393]) variable(1.1742401964288234)\n",
      "variable([[2.07153261]]) variable([4.86645736]) variable(1.130947281335902)\n",
      "variable([[2.08282962]]) variable([4.90871284]) variable(1.1037027660492316)\n",
      "variable([[2.09131795]]) variable([4.94238209]) variable(1.086544011239224)\n",
      "variable([[2.09767293]]) variable([4.96921596]) variable(1.0757285632165476)\n",
      "variable([[2.10241128]]) variable([4.99060704]) variable(1.0689057646903457)\n",
      "variable([[2.1059277]]) variable([5.00766322]) variable(1.064598052977042)\n",
      "variable([[2.10852317]]) variable([5.0212661]) variable(1.0618759595194345)\n",
      "variable([[2.11042676]]) variable([5.03211737]) variable(1.0601543388332906)\n",
      "variable([[2.11181245]]) variable([5.04077561]) variable(1.059064518022486)\n",
      "variable([[2.11281206]]) variable([5.04768562]) variable(1.058374023186869)\n",
      "variable([[2.11352521]]) variable([5.05320168]) variable(1.0579361409079033)\n",
      "variable([[2.11402698]]) variable([5.05760599]) variable(1.0576582021503207)\n",
      "variable([[2.11437377]]) variable([5.06112344]) variable(1.0574816235073754)\n",
      "variable([[2.11460778]]) variable([5.06393326]) variable(1.0573693373143267)\n",
      "variable([[2.11476047]]) variable([5.06617831]) variable(1.0572978688036483)\n",
      "variable([[2.11485517]]) variable([5.06797252]) variable(1.0572523381764114)\n",
      "variable([[2.11490911]]) variable([5.06940676]) variable(1.057223305126602)\n",
      "variable([[2.11493489]]) variable([5.0705535]) variable(1.0572047749024729)\n",
      "variable([[2.1149417]]) variable([5.07147059]) variable(1.057192937243938)\n",
      "variable([[2.11493615]]) variable([5.07220418]) variable(1.0571853681222083)\n",
      "variable([[2.11492296]]) variable([5.07279112]) variable(1.0571805239849006)\n",
      "variable([[2.11490544]]) variable([5.07326083]) variable(1.057177421035239)\n",
      "variable([[2.11488587]]) variable([5.0736368]) variable(1.0571754316643447)\n",
      "variable([[2.11486579]]) variable([5.07393782]) variable(1.0571741551246168)\n",
      "variable([[2.11484621]]) variable([5.07417887]) variable(1.0571733352929618)\n",
      "variable([[2.11482773]]) variable([5.07437194]) variable(1.0571728083295147)\n",
      "variable([[2.11481071]]) variable([5.07452663]) variable(1.0571724693332196)\n",
      "variable([[2.11479532]]) variable([5.07465057]) variable(1.0571722510799284)\n",
      "variable([[2.11478157]]) variable([5.07474992]) variable(1.0571721104523915)\n",
      "variable([[2.11476945]]) variable([5.07482956]) variable(1.0571720197715333)\n",
      "variable([[2.11475884]]) variable([5.07489341]) variable(1.0571719612537047)\n",
      "variable([[2.11474963]]) variable([5.07494462]) variable(1.0571719234634798)\n",
      "variable([[2.11474168]]) variable([5.0749857]) variable(1.0571718990415147)\n",
      "variable([[2.11473487]]) variable([5.07501866]) variable(1.0571718832478638)\n",
      "variable([[2.11472905]]) variable([5.07504511]) variable(1.0571718730272708)\n",
      "variable([[2.1147241]]) variable([5.07506634]) variable(1.0571718664088856)\n",
      "variable([[2.1147199]]) variable([5.07508338]) variable(1.0571718621204296)\n",
      "variable([[2.11471636]]) variable([5.07509707]) variable(1.0571718593399897)\n",
      "variable([[2.11471337]]) variable([5.07510806]) variable(1.0571718575362226)\n",
      "variable([[2.11471087]]) variable([5.07511688]) variable(1.0571718563653971)\n",
      "variable([[2.11470877]]) variable([5.07512397]) variable(1.057171855605001)\n",
      "variable([[2.11470701]]) variable([5.07512967]) variable(1.0571718551109015)\n",
      "variable([[2.11470554]]) variable([5.07513425]) variable(1.0571718547896791)\n",
      "variable([[2.11470432]]) variable([5.07513793]) variable(1.0571718545807458)\n",
      "variable([[2.1147033]]) variable([5.07514089]) variable(1.057171854444787)\n",
      "variable([[2.11470245]]) variable([5.07514327]) variable(1.0571718543562754)\n",
      "variable([[2.11470175]]) variable([5.07514519]) variable(1.0571718542986286)\n",
      "variable([[2.11470117]]) variable([5.07514673]) variable(1.0571718542610684)\n",
      "variable([[2.11470069]]) variable([5.07514797]) variable(1.0571718542365864)\n",
      "variable([[2.11470029]]) variable([5.07514896]) variable(1.0571718542206234)\n",
      "variable([[2.11469996]]) variable([5.07514977]) variable(1.057171854210211)\n",
      "variable([[2.11469969]]) variable([5.07515041]) variable(1.0571718542034167)\n",
      "variable([[2.11469946]]) variable([5.07515093]) variable(1.0571718541989825)\n",
      "variable([[2.11469928]]) variable([5.07515135]) variable(1.057171854196087)\n",
      "variable([[2.11469913]]) variable([5.07515169]) variable(1.0571718541941961)\n",
      "variable([[2.114699]]) variable([5.07515196]) variable(1.0571718541929611)\n",
      "variable([[2.1146989]]) variable([5.07515218]) variable(1.0571718541921542)\n",
      "variable([[2.11469882]]) variable([5.07515235]) variable(1.0571718541916264)\n",
      "variable([[2.11469875]]) variable([5.07515249]) variable(1.0571718541912813)\n",
      "variable([[2.11469869]]) variable([5.07515261]) variable(1.057171854191056)\n",
      "variable([[2.11469864]]) variable([5.0751527]) variable(1.0571718541909083)\n",
      "variable([[2.11469861]]) variable([5.07515278]) variable(1.057171854190812)\n",
      "variable([[2.11469857]]) variable([5.07515284]) variable(1.0571718541907489)\n",
      "variable([[2.11469855]]) variable([5.07515288]) variable(1.0571718541907074)\n",
      "variable([[2.11469853]]) variable([5.07515292]) variable(1.0571718541906805)\n",
      "variable([[2.11469851]]) variable([5.07515295]) variable(1.057171854190663)\n",
      "variable([[2.1146985]]) variable([5.07515298]) variable(1.0571718541906512)\n",
      "variable([[2.11469848]]) variable([5.075153]) variable(1.0571718541906439)\n",
      "variable([[2.11469848]]) variable([5.07515302]) variable(1.0571718541906387)\n",
      "variable([[2.11469847]]) variable([5.07515303]) variable(1.0571718541906354)\n",
      "variable([[2.11469846]]) variable([5.07515304]) variable(1.0571718541906332)\n",
      "variable([[2.11469846]]) variable([5.07515305]) variable(1.0571718541906319)\n",
      "variable([[2.11469845]]) variable([5.07515306]) variable(1.0571718541906308)\n",
      "variable([[2.11469845]]) variable([5.07515306]) variable(1.05717185419063)\n",
      "variable([[2.11469845]]) variable([5.07515307]) variable(1.0571718541906299)\n",
      "variable([[2.11469844]]) variable([5.07515307]) variable(1.0571718541906296)\n",
      "variable([[2.11469844]]) variable([5.07515307]) variable(1.0571718541906296)\n",
      "variable([[2.11469844]]) variable([5.07515307]) variable(1.0571718541906292)\n",
      "variable([[2.11469844]]) variable([5.07515308]) variable(1.0571718541906292)\n",
      "variable([[2.11469844]]) variable([5.07515308]) variable(1.0571718541906292)\n",
      "variable([[2.11469844]]) variable([5.07515308]) variable(1.0571718541906292)\n",
      "variable([[2.11469844]]) variable([5.07515308]) variable(1.0571718541906292)\n",
      "variable([[2.11469844]]) variable([5.07515308]) variable(1.0571718541906292)\n",
      "variable([[2.11469844]]) variable([5.07515308]) variable(1.057171854190629)\n",
      "variable([[2.11469844]]) variable([5.07515308]) variable(1.0571718541906288)\n",
      "variable([[2.11469843]]) variable([5.07515308]) variable(1.0571718541906292)\n",
      "variable([[2.11469843]]) variable([5.07515308]) variable(1.0571718541906292)\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(x0, x1):\n",
    "    diff = x0 - x1\n",
    "    return F.sum(diff ** 2) / len(diff)\n",
    "\n",
    "lr = 0.1\n",
    "iters = 100\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = mean_squared_error(y, y_pred)\n",
    "\n",
    "    W.cleargrad()\n",
    "    b.cleargrad()\n",
    "    loss.backward()\n",
    "\n",
    "    W.data -= lr * W.grad.data\n",
    "    b.data -= lr * b.grad.data\n",
    "    print(W, b, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Function):\n",
    "    def forward(self, x0, x1):\n",
    "        diff = x0 - x1\n",
    "        y = (diff ** 2).sum() / len(diff)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x0, x1 = self.inputs\n",
    "        diff = x0 - x1\n",
    "        gx0 = gy * diff * (2. / len(diff))\n",
    "        gx1 = -gx0\n",
    "        return gx0, gx1\n",
    "\n",
    "def mean_squared_error(x0, x1):\n",
    "    return MeanSquaredError()(x0, x1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 43 신경망\n",
    "\n",
    "- Linear transformation == Affine transformation:\n",
    "    \n",
    "    $y = xW$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_simple(x, W, b=None):\n",
    "    t = matmul(x, W)\n",
    "    if b is None:\n",
    "        return t\n",
    "    \n",
    "    y = t + b\n",
    "    t.data = None\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2084bbd2fd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 비선형 데이터셋\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_simple(x):\n",
    "    x = as_variable(x)\n",
    "    y = 1 / (1 + exp(-x))\n",
    "    return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 구현 예\n",
    "```\n",
    "def predict(x):\n",
    "    y = F.linear(x, W1, b1)\n",
    "    y = F.sigmoid(y)\n",
    "    y = F.linear(y, W2, b2)\n",
    "    return y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8473695850105871)\n",
      "variable(0.2514286285183607)\n",
      "variable(0.2475948546674987)\n",
      "variable(0.23786120447054812)\n",
      "variable(0.21222231333102934)\n",
      "variable(0.16742181117834157)\n",
      "variable(0.09681932619992645)\n",
      "variable(0.0784952829060233)\n",
      "variable(0.07749729552991157)\n",
      "variable(0.07722132399559316)\n"
     ]
    }
   ],
   "source": [
    "# 실제 데이터셋으로 신경망 학습 예제\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "# 가중치 초기화\n",
    "I, H, O = 1, 10, 1\n",
    "W1 = Variable(0.01 * np.random.randn(I, H))\n",
    "b1 = Variable(np.zeros(H))\n",
    "W2 = Variable(0.01 * np.random.randn(H, O))\n",
    "b2 = Variable(np.zeros(O))\n",
    "\n",
    "# 신경망 추론\n",
    "def predict(x):\n",
    "    y = F.linear(x, W1, b1)\n",
    "    y = F.sigmoid(y)\n",
    "    y = F.linear(y, W2, b2)\n",
    "    return y\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "# 신경망 학습\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    W1.cleargrad()\n",
    "    b1.cleargrad()\n",
    "    W2.cleargrad()\n",
    "    b2.cleargrad()\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data -= lr * W1.grad.data\n",
    "    b1.data -= lr * b1.grad.data\n",
    "    W2.data -= lr * W2.grad.data\n",
    "    b2.data -= lr * b2.grad.data\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 44 매개변수를 모아두는 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Variable):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Variable과 기능은 같지만 구별할 수 있다.\n",
    "x = Variable(np.array(1.0))\n",
    "p = Parameter(np.array(2.0))\n",
    "y = x * p\n",
    "\n",
    "print(isinstance(p, Parameter))\n",
    "print(isinstance(x, Parameter))\n",
    "print(isinstance(y, Parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 클래스 구현\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        # Layer 클래스가 갖는 매개변수를 인스턴스 변수 _params에 모아둘 수 있다.\n",
    "        self._params = set()\n",
    "    \n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Parameter):\n",
    "            self._params.add(name)\n",
    "        super().__setattr__(name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p2', 'p1'}\n",
      "-----------\n",
      "p2 variable(2)\n",
      "p1 variable(1)\n"
     ]
    }
   ],
   "source": [
    "layer = Layer()\n",
    "\n",
    "layer.p1 = Parameter(np.array(1))\n",
    "layer.p2 = Parameter(np.array(2))\n",
    "layer.p3 = Variable(np.array(3))\n",
    "layer.p4 = 'test'\n",
    "\n",
    "print(layer._params)\n",
    "print('-----------')\n",
    "\n",
    "for name in layer._params:\n",
    "    print(name, layer.__dict__[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weakref\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        # Layer 클래스가 갖는 매개변수를 인스턴스 변수 _params에 모아둘 수 있다.\n",
    "        self._params = set()\n",
    "    \n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Parameter):\n",
    "            self._params.add(name)\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def __call__(self, *inputs):\n",
    "        outputs = self.forward(*inputs)\n",
    "        if not isinstance(outputs, tuple):\n",
    "            outputs = (outputs,)\n",
    "        self.inputs = [weakref.ref(x) for x in inputs]\n",
    "        self.outputs = [weakref.ref(y) for x in outputs]\n",
    "        return outputs if len(outputs) > 1 else outputs[0]\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def params(self):\n",
    "        for name in self._params:\n",
    "            yield self.__dict__[name]\n",
    "        \n",
    "    def cleargrads(self):\n",
    "        for param in self.params():\n",
    "            param.cleargrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, in_size, out_size, nobias=False, dtype=np.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        I, O = in_size, out_size\n",
    "        W_data = np.random.randn(I, O).astype(dtype) * np.sqrt(1 / I)\n",
    "        self.W = Parameter(W_data, name='W')\n",
    "        if nobias:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = Parameter(np.zeros(0, dtype=dtype), name='b')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = F.linear(x, self.W, self.b)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선된 버전\n",
    "class Linear(Layer):\n",
    "    def __init__(self, out_size, nobias=False, dtype=np.float32, in_size=None):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.W = Parameter(None, name='W')\n",
    "        if self.in_size is not None:\n",
    "            self._init_W()\n",
    "        \n",
    "        if nobias:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = Parameter(np.zeros(out_size, dtype=dtype), name='b')\n",
    "    \n",
    "    def _init_W(self):\n",
    "        I, O = self.in_size, self.out_size\n",
    "        W_data = np.random.randn(I, O).astype(self.dtype) * np.sqrt(1 / I)\n",
    "        self.W.data = W_data\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 데이터를 흘려보내는 시점에 가중치 초기화\n",
    "        if self.W.data is None:\n",
    "            self.in_size = x.shape[1]\n",
    "            self._init_W()\n",
    "\n",
    "        y = F.linear(x, self.W, self.b)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350174)\n",
      "variable(0.12311905720649356)\n",
      "variable(0.07888166506355153)\n",
      "variable(0.07655073683421637)\n",
      "variable(0.07637803086238228)\n",
      "variable(0.07618764131185567)\n"
     ]
    }
   ],
   "source": [
    "import dezero.layers as L\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "l1 = L.Linear(10)\n",
    "l2 = L.Linear(1)\n",
    "\n",
    "def predict(x):\n",
    "    y = l1(x)\n",
    "    y = F.sigmoid(y)\n",
    "    y = l2(y)\n",
    "    return y\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "    l1.cleargrads()\n",
    "    l2.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    for l in [l1, l2]:\n",
    "        for p in l.params():\n",
    "            # print(p.data)\n",
    "            p.data -= lr * p.grad.data\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 45 계층을 모아두는 계층\n",
    "\n",
    "Layer 안에 다른 Layer가 들어갈 수 있도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self._params = set()\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, (Parameter, Layer)): # Layer 추가\n",
    "            self._params.add(name)\n",
    "        super().__setattr__(name, value)\n",
    "    \n",
    "    def params(self):\n",
    "        for name in self._params:\n",
    "            obj = self.__dict__[name]\n",
    "\n",
    "            if isinstance(obj, Layer): # Layer에서 매개변수 꺼내기\n",
    "                yield from obj.params() #\n",
    "            else:\n",
    "                yield obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(None)\n",
      "variable([0. 0. 0.])\n",
      "variable(None)\n",
      "variable([0. 0. 0. 0. 0.])\n"
     ]
    }
   ],
   "source": [
    "from dezero.layers import Layer\n",
    "\n",
    "model = Layer()\n",
    "model.l1 = L.Linear(5)\n",
    "model.l2 = L.Linear(3)\n",
    "\n",
    "def predict(model, x):\n",
    "    y = model.l1(x)\n",
    "    y = F.sigmoid(y)\n",
    "    y = model.l2(y)\n",
    "    return y\n",
    "\n",
    "for p in model.params():\n",
    "    print(p)\n",
    "\n",
    "model.cleargrads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 클래스 상속하여 모델 전체를 하나의 클래스로 정의\n",
    "class TwoLayerNet(Layer):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 클래스 정의\n",
    "\n",
    "모델이란? '사물의 본질을 단순하게 표현한 것'\n",
    "\n",
    "머신러닝 모델 또한 '복잡한 패턴이나 규칙이 숨어 있는 현상을 수식으로 단순하게 표현한 것'이다. 신경망처럼 수식으로 표현할 수 있는 함수를 가리켜 모델이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Layer):\n",
    "    def plot(self, *inputs, to_file='model.png'):\n",
    "        y = self.forward(*inputs)\n",
    "        return utils.plot_dot_graph(y, verbose=True, to_file=to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TwoLayerNet with Model class\n",
    "class TwoLayerNet(Model):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "\n",
    "x = Variable(np.random.randn(5, 10), name='x')\n",
    "model = TwoLayerNet(100, 10)\n",
    "model.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350174)\n",
      "variable(0.12311905720649356)\n",
      "variable(0.07888166506355153)\n",
      "variable(0.07655073683421637)\n",
      "variable(0.07637803086238228)\n",
      "variable(0.07618764131185567)\n"
     ]
    }
   ],
   "source": [
    "# Model을 사용한 sin 함수 데이터셋 회귀\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "class TwoLayerNet(Model):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "    \n",
    "model = TwoLayerNet(hidden_size, 1)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.params():\n",
    "        p.data -= lr * p.grad.data\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 클래스\n",
    "class MLP(Model):\n",
    "    def __init__(self, fc_output_sizes, activation=F.sigmoid):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.layers = []\n",
    "\n",
    "        for i, out_size in enumerate(fc_output_sizes):\n",
    "            layer = L.Linear(out_size)\n",
    "            setattr(self, 'l' + str(i), layer)\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for l in self.layers[:-1]:\n",
    "            x = self.activation(l(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 46 Optimizer로 수행하는 매개변수 갱신\n",
    "\n",
    "경사하강법 외에 다양한 최적화 기법 구현을 위한 Optimizer 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        self.target = None\n",
    "        self.hooks = []\n",
    "    \n",
    "    def setup(self, target):\n",
    "        self.target = target\n",
    "        return self\n",
    "    \n",
    "    def update(self):\n",
    "        # None 이외의 매개변수를 리스트에 모아둠\n",
    "        params = [p for p in self.target.params() if p.grad is not None]\n",
    "        \n",
    "        # 전처리 (옵션)\n",
    "        for f in self.hooks:\n",
    "            f(params)\n",
    "\n",
    "        # 매개변수 갱신\n",
    "        for param in params:\n",
    "            self.update_one(param)\n",
    "        \n",
    "    def update_one(self, param):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def add_hook(self, f):\n",
    "        self.hooks.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD 클래스\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update_one(self, param):\n",
    "        param.data -= self.lr * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350174)\n",
      "variable(0.12311905720649356)\n",
      "variable(0.07888166506355153)\n",
      "variable(0.07655073683421637)\n",
      "variable(0.07637803086238228)\n",
      "variable(0.07618764131185567)\n"
     ]
    }
   ],
   "source": [
    "# SGD 클래스 활용\n",
    "from dezero import optimizers\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "model = MLP((hidden_size, 1))\n",
    "# optimizer = optimizers.SGD(lr)\n",
    "# optimizer.setup(model)\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.update() #\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l0 variable([[ 1.6524137  -2.1517513  -9.170103   -1.1727254  -1.1385859   3.307915\n",
      "            1.617087   -0.63725543 -2.1345348   1.4791305 ]])\n",
      "l1 variable([[ 1.0825084 ]\n",
      "          [-2.7484486 ]\n",
      "          [ 5.4915586 ]\n",
      "          [-1.3048114 ]\n",
      "          [-1.197905  ]\n",
      "          [ 0.32324767]\n",
      "          [ 1.4315618 ]\n",
      "          [-0.86709327]\n",
      "          [-2.6347868 ]\n",
      "          [ 1.2583766 ]])\n",
      "------\n",
      "W [[ 1.6524137  -2.1517513  -9.170103   -1.1727254  -1.1385859   3.307915\n",
      "   1.617087   -0.63725543 -2.1345348   1.4791305 ]]\n",
      "b [-0.9606268   0.49943432  4.5593724   0.16067517  0.20714144 -1.874649\n",
      " -0.5832339  -0.00763051  0.57859486 -0.5276554 ]\n",
      "W [[ 1.0825084 ]\n",
      " [-2.7484486 ]\n",
      " [ 5.4915586 ]\n",
      " [-1.3048114 ]\n",
      " [-1.197905  ]\n",
      " [ 0.32324767]\n",
      " [ 1.4315618 ]\n",
      " [-0.86709327]\n",
      " [-2.6347868 ]\n",
      " [ 1.2583766 ]]\n",
      "b [-0.91091275]\n"
     ]
    }
   ],
   "source": [
    "for name in model._params:\n",
    "    print(name, model.__dict__[name].W)\n",
    "\n",
    "# type(model.__dict__['l1'])\n",
    "\n",
    "# model.params().\n",
    "\n",
    "print('------')\n",
    "\n",
    "for p in model.params():\n",
    "    print(p.name, p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MomentumSGD\n",
    "class MomentumSGD(Optimizer):\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.vs = {}\n",
    "\n",
    "    def update_one(self, param):\n",
    "        v_key = id(param)\n",
    "        if v_key not in self.vs:\n",
    "            self.vs[v_key] = np.zeros_like(param.data)\n",
    "        \n",
    "        v = self.vs[v_key]\n",
    "        v *= self.momentum\n",
    "        v -= self.lr * param.grad.data\n",
    "        param.data += v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 47 소프트맥스 함수와 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1 2 3]\n",
      "          [1 2 3]\n",
      "          [4 5 6]])\n"
     ]
    }
   ],
   "source": [
    "# 슬라이스\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "indices = np.array([0, 0, 1])\n",
    "y = F.get_item(x, indices)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([4 5 6])\n",
      "variable([3 6])\n"
     ]
    }
   ],
   "source": [
    "import dezero.functions as F\n",
    "Variable.__getitem__ = F.get_item\n",
    "y = x[1]\n",
    "print(y)\n",
    "\n",
    "y = x[:, 2]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def softmax1d(x):\n",
    "    x = as_variable(x)\n",
    "    y = F.exp(x)\n",
    "    sum_y = F.sum(y)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[0.25337848 0.30853735 0.38821423]])\n",
      "variable([[0.31239678 0.33011233 0.35749089]])\n"
     ]
    }
   ],
   "source": [
    "model = MLP((10, 3))\n",
    "\n",
    "x = Variable(np.array([[0.2, -0.4]]))\n",
    "y = model(x)\n",
    "p = softmax1d(y)\n",
    "\n",
    "print(y)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 데이터 처리 가능한 softmax 함수\n",
    "from dezero.functions import *\n",
    "def softmax_simple(x, axis=1):\n",
    "    x = as_variable(x)\n",
    "    y = exp(x)\n",
    "    sum_y = sum(y, axis=axis, keepdims=True)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 엔트로피 함수 구현\n",
    "def softmax_cross_entropy_simple(x, t):\n",
    "    x, t = as_variable(x), as_variable(t)\n",
    "    N = x.shape[0]\n",
    "\n",
    "    p = softmax_simple(x)\n",
    "    p = clip(p, 1e-15, 1.0)\n",
    "    log_p = log(p)\n",
    "    tlog_p = log_p[np.arange(N), t.data]\n",
    "    y = -1 * sum(tlog_p) / N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(1.1784240941964543)\n"
     ]
    }
   ],
   "source": [
    "# 교차 엔트로피 함수 활용\n",
    "x = np.array([[0.2, -0.4], [0.3, 0.5], [1.3, -3.2], [2.1, 0.3]])\n",
    "t = np.array([2, 0, 1, 0])\n",
    "y = model(x)\n",
    "loss = F.softmax_cross_entropy(y, t)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 48 다중 클래스 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n",
      "(300,)\n",
      "[-0.12995958 -0.00324155] 1\n",
      "[ 0.3282343  -0.54941994] 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2084d313550>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "print(x.shape)\n",
    "print(t.shape)\n",
    "\n",
    "print(x[10], t[10])\n",
    "print(x[110], t[110])\n",
    "plt.scatter(x[:, 0], x[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.13\n",
      "epoch 2, loss 1.05\n",
      "epoch 3, loss 0.95\n",
      "epoch 4, loss 0.92\n",
      "epoch 5, loss 0.87\n",
      "epoch 6, loss 0.89\n",
      "epoch 7, loss 0.84\n",
      "epoch 8, loss 0.78\n",
      "epoch 9, loss 0.80\n",
      "epoch 10, loss 0.79\n",
      "epoch 11, loss 0.78\n",
      "epoch 12, loss 0.76\n",
      "epoch 13, loss 0.77\n",
      "epoch 14, loss 0.76\n",
      "epoch 15, loss 0.76\n",
      "epoch 16, loss 0.77\n",
      "epoch 17, loss 0.78\n",
      "epoch 18, loss 0.74\n",
      "epoch 19, loss 0.74\n",
      "epoch 20, loss 0.72\n",
      "epoch 21, loss 0.73\n",
      "epoch 22, loss 0.74\n",
      "epoch 23, loss 0.77\n",
      "epoch 24, loss 0.73\n",
      "epoch 25, loss 0.74\n",
      "epoch 26, loss 0.74\n",
      "epoch 27, loss 0.72\n",
      "epoch 28, loss 0.72\n",
      "epoch 29, loss 0.72\n",
      "epoch 30, loss 0.73\n",
      "epoch 31, loss 0.71\n",
      "epoch 32, loss 0.72\n",
      "epoch 33, loss 0.72\n",
      "epoch 34, loss 0.71\n",
      "epoch 35, loss 0.72\n",
      "epoch 36, loss 0.71\n",
      "epoch 37, loss 0.71\n",
      "epoch 38, loss 0.70\n",
      "epoch 39, loss 0.71\n",
      "epoch 40, loss 0.70\n",
      "epoch 41, loss 0.71\n",
      "epoch 42, loss 0.70\n",
      "epoch 43, loss 0.70\n",
      "epoch 44, loss 0.70\n",
      "epoch 45, loss 0.69\n",
      "epoch 46, loss 0.69\n",
      "epoch 47, loss 0.71\n",
      "epoch 48, loss 0.70\n",
      "epoch 49, loss 0.69\n",
      "epoch 50, loss 0.69\n",
      "epoch 51, loss 0.68\n",
      "epoch 52, loss 0.67\n",
      "epoch 53, loss 0.68\n",
      "epoch 54, loss 0.70\n",
      "epoch 55, loss 0.68\n",
      "epoch 56, loss 0.66\n",
      "epoch 57, loss 0.67\n",
      "epoch 58, loss 0.66\n",
      "epoch 59, loss 0.64\n",
      "epoch 60, loss 0.64\n",
      "epoch 61, loss 0.64\n",
      "epoch 62, loss 0.63\n",
      "epoch 63, loss 0.63\n",
      "epoch 64, loss 0.61\n",
      "epoch 65, loss 0.61\n",
      "epoch 66, loss 0.60\n",
      "epoch 67, loss 0.62\n",
      "epoch 68, loss 0.59\n",
      "epoch 69, loss 0.60\n",
      "epoch 70, loss 0.57\n",
      "epoch 71, loss 0.58\n",
      "epoch 72, loss 0.57\n",
      "epoch 73, loss 0.56\n",
      "epoch 74, loss 0.56\n",
      "epoch 75, loss 0.55\n",
      "epoch 76, loss 0.55\n",
      "epoch 77, loss 0.55\n",
      "epoch 78, loss 0.54\n",
      "epoch 79, loss 0.53\n",
      "epoch 80, loss 0.53\n",
      "epoch 81, loss 0.52\n",
      "epoch 82, loss 0.53\n",
      "epoch 83, loss 0.52\n",
      "epoch 84, loss 0.49\n",
      "epoch 85, loss 0.50\n",
      "epoch 86, loss 0.49\n",
      "epoch 87, loss 0.49\n",
      "epoch 88, loss 0.48\n",
      "epoch 89, loss 0.47\n",
      "epoch 90, loss 0.47\n",
      "epoch 91, loss 0.46\n",
      "epoch 92, loss 0.46\n",
      "epoch 93, loss 0.45\n",
      "epoch 94, loss 0.44\n",
      "epoch 95, loss 0.45\n",
      "epoch 96, loss 0.44\n",
      "epoch 97, loss 0.43\n",
      "epoch 98, loss 0.43\n",
      "epoch 99, loss 0.42\n",
      "epoch 100, loss 0.43\n",
      "epoch 101, loss 0.42\n",
      "epoch 102, loss 0.41\n",
      "epoch 103, loss 0.42\n",
      "epoch 104, loss 0.41\n",
      "epoch 105, loss 0.40\n",
      "epoch 106, loss 0.40\n",
      "epoch 107, loss 0.40\n",
      "epoch 108, loss 0.39\n",
      "epoch 109, loss 0.38\n",
      "epoch 110, loss 0.39\n",
      "epoch 111, loss 0.38\n",
      "epoch 112, loss 0.38\n",
      "epoch 113, loss 0.38\n",
      "epoch 114, loss 0.36\n",
      "epoch 115, loss 0.36\n",
      "epoch 116, loss 0.36\n",
      "epoch 117, loss 0.36\n",
      "epoch 118, loss 0.36\n",
      "epoch 119, loss 0.35\n",
      "epoch 120, loss 0.35\n",
      "epoch 121, loss 0.36\n",
      "epoch 122, loss 0.34\n",
      "epoch 123, loss 0.35\n",
      "epoch 124, loss 0.33\n",
      "epoch 125, loss 0.33\n",
      "epoch 126, loss 0.32\n",
      "epoch 127, loss 0.34\n",
      "epoch 128, loss 0.32\n",
      "epoch 129, loss 0.33\n",
      "epoch 130, loss 0.31\n",
      "epoch 131, loss 0.30\n",
      "epoch 132, loss 0.31\n",
      "epoch 133, loss 0.31\n",
      "epoch 134, loss 0.30\n",
      "epoch 135, loss 0.29\n",
      "epoch 136, loss 0.29\n",
      "epoch 137, loss 0.29\n",
      "epoch 138, loss 0.28\n",
      "epoch 139, loss 0.29\n",
      "epoch 140, loss 0.28\n",
      "epoch 141, loss 0.27\n",
      "epoch 142, loss 0.27\n",
      "epoch 143, loss 0.28\n",
      "epoch 144, loss 0.27\n",
      "epoch 145, loss 0.27\n",
      "epoch 146, loss 0.26\n",
      "epoch 147, loss 0.26\n",
      "epoch 148, loss 0.26\n",
      "epoch 149, loss 0.26\n",
      "epoch 150, loss 0.25\n",
      "epoch 151, loss 0.25\n",
      "epoch 152, loss 0.25\n",
      "epoch 153, loss 0.24\n",
      "epoch 154, loss 0.24\n",
      "epoch 155, loss 0.24\n",
      "epoch 156, loss 0.24\n",
      "epoch 157, loss 0.24\n",
      "epoch 158, loss 0.24\n",
      "epoch 159, loss 0.23\n",
      "epoch 160, loss 0.23\n",
      "epoch 161, loss 0.23\n",
      "epoch 162, loss 0.23\n",
      "epoch 163, loss 0.23\n",
      "epoch 164, loss 0.22\n",
      "epoch 165, loss 0.22\n",
      "epoch 166, loss 0.22\n",
      "epoch 167, loss 0.21\n",
      "epoch 168, loss 0.22\n",
      "epoch 169, loss 0.22\n",
      "epoch 170, loss 0.21\n",
      "epoch 171, loss 0.21\n",
      "epoch 172, loss 0.22\n",
      "epoch 173, loss 0.22\n",
      "epoch 174, loss 0.21\n",
      "epoch 175, loss 0.21\n",
      "epoch 176, loss 0.20\n",
      "epoch 177, loss 0.21\n",
      "epoch 178, loss 0.20\n",
      "epoch 179, loss 0.20\n",
      "epoch 180, loss 0.20\n",
      "epoch 181, loss 0.20\n",
      "epoch 182, loss 0.19\n",
      "epoch 183, loss 0.20\n",
      "epoch 184, loss 0.19\n",
      "epoch 185, loss 0.19\n",
      "epoch 186, loss 0.19\n",
      "epoch 187, loss 0.19\n",
      "epoch 188, loss 0.19\n",
      "epoch 189, loss 0.19\n",
      "epoch 190, loss 0.19\n",
      "epoch 191, loss 0.19\n",
      "epoch 192, loss 0.19\n",
      "epoch 193, loss 0.18\n",
      "epoch 194, loss 0.19\n",
      "epoch 195, loss 0.18\n",
      "epoch 196, loss 0.18\n",
      "epoch 197, loss 0.18\n",
      "epoch 198, loss 0.18\n",
      "epoch 199, loss 0.19\n",
      "epoch 200, loss 0.18\n",
      "epoch 201, loss 0.17\n",
      "epoch 202, loss 0.18\n",
      "epoch 203, loss 0.18\n",
      "epoch 204, loss 0.17\n",
      "epoch 205, loss 0.18\n",
      "epoch 206, loss 0.17\n",
      "epoch 207, loss 0.17\n",
      "epoch 208, loss 0.17\n",
      "epoch 209, loss 0.17\n",
      "epoch 210, loss 0.17\n",
      "epoch 211, loss 0.17\n",
      "epoch 212, loss 0.17\n",
      "epoch 213, loss 0.18\n",
      "epoch 214, loss 0.17\n",
      "epoch 215, loss 0.17\n",
      "epoch 216, loss 0.17\n",
      "epoch 217, loss 0.17\n",
      "epoch 218, loss 0.17\n",
      "epoch 219, loss 0.16\n",
      "epoch 220, loss 0.17\n",
      "epoch 221, loss 0.16\n",
      "epoch 222, loss 0.16\n",
      "epoch 223, loss 0.16\n",
      "epoch 224, loss 0.16\n",
      "epoch 225, loss 0.16\n",
      "epoch 226, loss 0.16\n",
      "epoch 227, loss 0.17\n",
      "epoch 228, loss 0.18\n",
      "epoch 229, loss 0.16\n",
      "epoch 230, loss 0.16\n",
      "epoch 231, loss 0.15\n",
      "epoch 232, loss 0.16\n",
      "epoch 233, loss 0.17\n",
      "epoch 234, loss 0.16\n",
      "epoch 235, loss 0.16\n",
      "epoch 236, loss 0.15\n",
      "epoch 237, loss 0.16\n",
      "epoch 238, loss 0.16\n",
      "epoch 239, loss 0.16\n",
      "epoch 240, loss 0.16\n",
      "epoch 241, loss 0.15\n",
      "epoch 242, loss 0.15\n",
      "epoch 243, loss 0.15\n",
      "epoch 244, loss 0.15\n",
      "epoch 245, loss 0.15\n",
      "epoch 246, loss 0.15\n",
      "epoch 247, loss 0.15\n",
      "epoch 248, loss 0.15\n",
      "epoch 249, loss 0.15\n",
      "epoch 250, loss 0.15\n",
      "epoch 251, loss 0.15\n",
      "epoch 252, loss 0.15\n",
      "epoch 253, loss 0.15\n",
      "epoch 254, loss 0.15\n",
      "epoch 255, loss 0.15\n",
      "epoch 256, loss 0.15\n",
      "epoch 257, loss 0.14\n",
      "epoch 258, loss 0.15\n",
      "epoch 259, loss 0.14\n",
      "epoch 260, loss 0.15\n",
      "epoch 261, loss 0.15\n",
      "epoch 262, loss 0.15\n",
      "epoch 263, loss 0.14\n",
      "epoch 264, loss 0.14\n",
      "epoch 265, loss 0.14\n",
      "epoch 266, loss 0.14\n",
      "epoch 267, loss 0.14\n",
      "epoch 268, loss 0.14\n",
      "epoch 269, loss 0.14\n",
      "epoch 270, loss 0.14\n",
      "epoch 271, loss 0.14\n",
      "epoch 272, loss 0.14\n",
      "epoch 273, loss 0.14\n",
      "epoch 274, loss 0.14\n",
      "epoch 275, loss 0.14\n",
      "epoch 276, loss 0.14\n",
      "epoch 277, loss 0.14\n",
      "epoch 278, loss 0.14\n",
      "epoch 279, loss 0.14\n",
      "epoch 280, loss 0.13\n",
      "epoch 281, loss 0.13\n",
      "epoch 282, loss 0.14\n",
      "epoch 283, loss 0.13\n",
      "epoch 284, loss 0.13\n",
      "epoch 285, loss 0.13\n",
      "epoch 286, loss 0.13\n",
      "epoch 287, loss 0.14\n",
      "epoch 288, loss 0.13\n",
      "epoch 289, loss 0.13\n",
      "epoch 290, loss 0.13\n",
      "epoch 291, loss 0.13\n",
      "epoch 292, loss 0.13\n",
      "epoch 293, loss 0.14\n",
      "epoch 294, loss 0.13\n",
      "epoch 295, loss 0.13\n",
      "epoch 296, loss 0.13\n",
      "epoch 297, loss 0.13\n",
      "epoch 298, loss 0.12\n",
      "epoch 299, loss 0.13\n",
      "epoch 300, loss 0.13\n"
     ]
    }
   ],
   "source": [
    "# 다중 분류 학습\n",
    "import math\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "# 데이터 읽기 / 모델, 옵티마이저 생성\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # 데이터셋의 인덱스 뒤섞기\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # 미니 배치 생성\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "\n",
    "        # 기울기 산출 / 매개변수 갱신\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # 에포크마다 학습 경과 출력\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch+1, avg_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 49 Dataset 클래스와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train=True):\n",
    "        self.train = train\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index) # index는 정수(스칼라)만 지원\n",
    "        if self.label is None:\n",
    "            return self.data[index], None\n",
    "        else:\n",
    "            return self.data[index], self.label[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def prepare(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.datasets import get_spiral\n",
    "class Spiral(Dataset):\n",
    "    def prepare(self):\n",
    "        self.data, self.label = get_spiral(self.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.13981389, -0.00721657], dtype=float32), 1)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "print(train_set[0])\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 큰 데이터셋\n",
    "class BigData(Dataset):\n",
    "    def __getitem__(index):\n",
    "        x = np.load('data/{}.npy'.format(index))\n",
    "        t = np.load('label/{}.npy'.format(index))\n",
    "        return x, t\n",
    "    \n",
    "    def __len__():\n",
    "        return 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 이어붙이기\n",
    "train_set = dezero.datasets.Spiral()\n",
    "\n",
    "batch_index = [0, 1, 2]\n",
    "batch = [train_set[i] for i in batch_index]\n",
    "\n",
    "x = np.array([example[0] for example in batch])\n",
    "t = np.array([example[1] for example in batch])\n",
    "\n",
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.35\n",
      "epoch 2, loss 1.06\n",
      "epoch 3, loss 0.98\n",
      "epoch 4, loss 0.90\n",
      "epoch 5, loss 0.88\n",
      "epoch 6, loss 0.84\n",
      "epoch 7, loss 0.84\n",
      "epoch 8, loss 0.81\n",
      "epoch 9, loss 0.79\n",
      "epoch 10, loss 0.77\n",
      "epoch 11, loss 0.80\n",
      "epoch 12, loss 0.77\n",
      "epoch 13, loss 0.76\n",
      "epoch 14, loss 0.76\n",
      "epoch 15, loss 0.76\n",
      "epoch 16, loss 0.76\n",
      "epoch 17, loss 0.76\n",
      "epoch 18, loss 0.80\n",
      "epoch 19, loss 0.78\n",
      "epoch 20, loss 0.77\n",
      "epoch 21, loss 0.76\n",
      "epoch 22, loss 0.74\n",
      "epoch 23, loss 0.79\n",
      "epoch 24, loss 0.77\n",
      "epoch 25, loss 0.75\n",
      "epoch 26, loss 0.77\n",
      "epoch 27, loss 0.77\n",
      "epoch 28, loss 0.74\n",
      "epoch 29, loss 0.77\n",
      "epoch 30, loss 0.75\n",
      "epoch 31, loss 0.75\n",
      "epoch 32, loss 0.77\n",
      "epoch 33, loss 0.74\n",
      "epoch 34, loss 0.75\n",
      "epoch 35, loss 0.76\n",
      "epoch 36, loss 0.74\n",
      "epoch 37, loss 0.80\n",
      "epoch 38, loss 0.73\n",
      "epoch 39, loss 0.74\n",
      "epoch 40, loss 0.74\n",
      "epoch 41, loss 0.78\n",
      "epoch 42, loss 0.75\n",
      "epoch 43, loss 0.72\n",
      "epoch 44, loss 0.74\n",
      "epoch 45, loss 0.71\n",
      "epoch 46, loss 0.72\n",
      "epoch 47, loss 0.74\n",
      "epoch 48, loss 0.71\n",
      "epoch 49, loss 0.71\n",
      "epoch 50, loss 0.71\n",
      "epoch 51, loss 0.73\n",
      "epoch 52, loss 0.75\n",
      "epoch 53, loss 0.73\n",
      "epoch 54, loss 0.71\n",
      "epoch 55, loss 0.70\n",
      "epoch 56, loss 0.70\n",
      "epoch 57, loss 0.73\n",
      "epoch 58, loss 0.71\n",
      "epoch 59, loss 0.70\n",
      "epoch 60, loss 0.70\n",
      "epoch 61, loss 0.68\n",
      "epoch 62, loss 0.68\n",
      "epoch 63, loss 0.70\n",
      "epoch 64, loss 0.66\n",
      "epoch 65, loss 0.67\n",
      "epoch 66, loss 0.68\n",
      "epoch 67, loss 0.67\n",
      "epoch 68, loss 0.69\n",
      "epoch 69, loss 0.66\n",
      "epoch 70, loss 0.66\n",
      "epoch 71, loss 0.65\n",
      "epoch 72, loss 0.64\n",
      "epoch 73, loss 0.66\n",
      "epoch 74, loss 0.66\n",
      "epoch 75, loss 0.63\n",
      "epoch 76, loss 0.60\n",
      "epoch 77, loss 0.62\n",
      "epoch 78, loss 0.61\n",
      "epoch 79, loss 0.61\n",
      "epoch 80, loss 0.62\n",
      "epoch 81, loss 0.61\n",
      "epoch 82, loss 0.61\n",
      "epoch 83, loss 0.58\n",
      "epoch 84, loss 0.60\n",
      "epoch 85, loss 0.57\n",
      "epoch 86, loss 0.56\n",
      "epoch 87, loss 0.56\n",
      "epoch 88, loss 0.56\n",
      "epoch 89, loss 0.56\n",
      "epoch 90, loss 0.56\n",
      "epoch 91, loss 0.54\n",
      "epoch 92, loss 0.54\n",
      "epoch 93, loss 0.54\n",
      "epoch 94, loss 0.52\n",
      "epoch 95, loss 0.53\n",
      "epoch 96, loss 0.51\n",
      "epoch 97, loss 0.53\n",
      "epoch 98, loss 0.52\n",
      "epoch 99, loss 0.49\n",
      "epoch 100, loss 0.48\n",
      "epoch 101, loss 0.49\n",
      "epoch 102, loss 0.50\n",
      "epoch 103, loss 0.49\n",
      "epoch 104, loss 0.48\n",
      "epoch 105, loss 0.49\n",
      "epoch 106, loss 0.46\n",
      "epoch 107, loss 0.45\n",
      "epoch 108, loss 0.46\n",
      "epoch 109, loss 0.44\n",
      "epoch 110, loss 0.45\n",
      "epoch 111, loss 0.44\n",
      "epoch 112, loss 0.45\n",
      "epoch 113, loss 0.44\n",
      "epoch 114, loss 0.41\n",
      "epoch 115, loss 0.41\n",
      "epoch 116, loss 0.41\n",
      "epoch 117, loss 0.39\n",
      "epoch 118, loss 0.40\n",
      "epoch 119, loss 0.39\n",
      "epoch 120, loss 0.39\n",
      "epoch 121, loss 0.40\n",
      "epoch 122, loss 0.36\n",
      "epoch 123, loss 0.39\n",
      "epoch 124, loss 0.36\n",
      "epoch 125, loss 0.35\n",
      "epoch 126, loss 0.34\n",
      "epoch 127, loss 0.35\n",
      "epoch 128, loss 0.33\n",
      "epoch 129, loss 0.35\n",
      "epoch 130, loss 0.32\n",
      "epoch 131, loss 0.31\n",
      "epoch 132, loss 0.33\n",
      "epoch 133, loss 0.31\n",
      "epoch 134, loss 0.30\n",
      "epoch 135, loss 0.29\n",
      "epoch 136, loss 0.29\n",
      "epoch 137, loss 0.29\n",
      "epoch 138, loss 0.28\n",
      "epoch 139, loss 0.29\n",
      "epoch 140, loss 0.27\n",
      "epoch 141, loss 0.27\n",
      "epoch 142, loss 0.27\n",
      "epoch 143, loss 0.28\n",
      "epoch 144, loss 0.26\n",
      "epoch 145, loss 0.26\n",
      "epoch 146, loss 0.25\n",
      "epoch 147, loss 0.25\n",
      "epoch 148, loss 0.25\n",
      "epoch 149, loss 0.25\n",
      "epoch 150, loss 0.25\n",
      "epoch 151, loss 0.25\n",
      "epoch 152, loss 0.24\n",
      "epoch 153, loss 0.23\n",
      "epoch 154, loss 0.23\n",
      "epoch 155, loss 0.23\n",
      "epoch 156, loss 0.23\n",
      "epoch 157, loss 0.22\n",
      "epoch 158, loss 0.22\n",
      "epoch 159, loss 0.22\n",
      "epoch 160, loss 0.21\n",
      "epoch 161, loss 0.21\n",
      "epoch 162, loss 0.21\n",
      "epoch 163, loss 0.21\n",
      "epoch 164, loss 0.21\n",
      "epoch 165, loss 0.21\n",
      "epoch 166, loss 0.20\n",
      "epoch 167, loss 0.20\n",
      "epoch 168, loss 0.21\n",
      "epoch 169, loss 0.21\n",
      "epoch 170, loss 0.19\n",
      "epoch 171, loss 0.20\n",
      "epoch 172, loss 0.21\n",
      "epoch 173, loss 0.20\n",
      "epoch 174, loss 0.19\n",
      "epoch 175, loss 0.19\n",
      "epoch 176, loss 0.19\n",
      "epoch 177, loss 0.19\n",
      "epoch 178, loss 0.18\n",
      "epoch 179, loss 0.19\n",
      "epoch 180, loss 0.18\n",
      "epoch 181, loss 0.18\n",
      "epoch 182, loss 0.18\n",
      "epoch 183, loss 0.18\n",
      "epoch 184, loss 0.18\n",
      "epoch 185, loss 0.18\n",
      "epoch 186, loss 0.17\n",
      "epoch 187, loss 0.18\n",
      "epoch 188, loss 0.17\n",
      "epoch 189, loss 0.18\n",
      "epoch 190, loss 0.17\n",
      "epoch 191, loss 0.17\n",
      "epoch 192, loss 0.17\n",
      "epoch 193, loss 0.16\n",
      "epoch 194, loss 0.17\n",
      "epoch 195, loss 0.17\n",
      "epoch 196, loss 0.16\n",
      "epoch 197, loss 0.16\n",
      "epoch 198, loss 0.16\n",
      "epoch 199, loss 0.17\n",
      "epoch 200, loss 0.16\n",
      "epoch 201, loss 0.15\n",
      "epoch 202, loss 0.16\n",
      "epoch 203, loss 0.17\n",
      "epoch 204, loss 0.15\n",
      "epoch 205, loss 0.16\n",
      "epoch 206, loss 0.15\n",
      "epoch 207, loss 0.15\n",
      "epoch 208, loss 0.16\n",
      "epoch 209, loss 0.15\n",
      "epoch 210, loss 0.16\n",
      "epoch 211, loss 0.15\n",
      "epoch 212, loss 0.15\n",
      "epoch 213, loss 0.16\n",
      "epoch 214, loss 0.15\n",
      "epoch 215, loss 0.15\n",
      "epoch 216, loss 0.15\n",
      "epoch 217, loss 0.15\n",
      "epoch 218, loss 0.15\n",
      "epoch 219, loss 0.14\n",
      "epoch 220, loss 0.15\n",
      "epoch 221, loss 0.15\n",
      "epoch 222, loss 0.14\n",
      "epoch 223, loss 0.14\n",
      "epoch 224, loss 0.14\n",
      "epoch 225, loss 0.14\n",
      "epoch 226, loss 0.14\n",
      "epoch 227, loss 0.16\n",
      "epoch 228, loss 0.17\n",
      "epoch 229, loss 0.14\n",
      "epoch 230, loss 0.14\n",
      "epoch 231, loss 0.14\n",
      "epoch 232, loss 0.15\n",
      "epoch 233, loss 0.15\n",
      "epoch 234, loss 0.14\n",
      "epoch 235, loss 0.14\n",
      "epoch 236, loss 0.13\n",
      "epoch 237, loss 0.14\n",
      "epoch 238, loss 0.14\n",
      "epoch 239, loss 0.14\n",
      "epoch 240, loss 0.14\n",
      "epoch 241, loss 0.14\n",
      "epoch 242, loss 0.13\n",
      "epoch 243, loss 0.13\n",
      "epoch 244, loss 0.14\n",
      "epoch 245, loss 0.13\n",
      "epoch 246, loss 0.13\n",
      "epoch 247, loss 0.13\n",
      "epoch 248, loss 0.13\n",
      "epoch 249, loss 0.13\n",
      "epoch 250, loss 0.13\n",
      "epoch 251, loss 0.13\n",
      "epoch 252, loss 0.13\n",
      "epoch 253, loss 0.13\n",
      "epoch 254, loss 0.14\n",
      "epoch 255, loss 0.13\n",
      "epoch 256, loss 0.13\n",
      "epoch 257, loss 0.12\n",
      "epoch 258, loss 0.13\n",
      "epoch 259, loss 0.13\n",
      "epoch 260, loss 0.13\n",
      "epoch 261, loss 0.13\n",
      "epoch 262, loss 0.13\n",
      "epoch 263, loss 0.13\n",
      "epoch 264, loss 0.13\n",
      "epoch 265, loss 0.12\n",
      "epoch 266, loss 0.12\n",
      "epoch 267, loss 0.12\n",
      "epoch 268, loss 0.13\n",
      "epoch 269, loss 0.13\n",
      "epoch 270, loss 0.13\n",
      "epoch 271, loss 0.13\n",
      "epoch 272, loss 0.12\n",
      "epoch 273, loss 0.12\n",
      "epoch 274, loss 0.12\n",
      "epoch 275, loss 0.12\n",
      "epoch 276, loss 0.12\n",
      "epoch 277, loss 0.12\n",
      "epoch 278, loss 0.13\n",
      "epoch 279, loss 0.12\n",
      "epoch 280, loss 0.12\n",
      "epoch 281, loss 0.12\n",
      "epoch 282, loss 0.12\n",
      "epoch 283, loss 0.11\n",
      "epoch 284, loss 0.13\n",
      "epoch 285, loss 0.12\n",
      "epoch 286, loss 0.12\n",
      "epoch 287, loss 0.12\n",
      "epoch 288, loss 0.12\n",
      "epoch 289, loss 0.11\n",
      "epoch 290, loss 0.12\n",
      "epoch 291, loss 0.12\n",
      "epoch 292, loss 0.12\n",
      "epoch 293, loss 0.13\n",
      "epoch 294, loss 0.12\n",
      "epoch 295, loss 0.12\n",
      "epoch 296, loss 0.12\n",
      "epoch 297, loss 0.12\n",
      "epoch 298, loss 0.11\n",
      "epoch 299, loss 0.11\n",
      "epoch 300, loss 0.12\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "train_set = dezero.datasets.Spiral()\n",
    "model = MLP((hidden_size, 10))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(train_set)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # 미니배치 꺼내기\n",
    "        batch_index = index[i * batch_size:(i+1) * batch_size]\n",
    "        batch = [train_set[i] for i in batch_index]\n",
    "        batch_x = np.array([example[0] for example in batch])\n",
    "        batch_t = np.array([example[1] for example in batch])\n",
    "\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "    \n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 (ex. Data augmentation)\n",
    "class Dataset:\n",
    "    def __init__(self, train=True, transform=None, target_transform=None):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if self.transform is None:\n",
    "            self.transform = lambda x: x\n",
    "        if self.target_transform is None:\n",
    "            self.target_transform = lambda x: x\n",
    "        \n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index)\n",
    "        if self.label is None:\n",
    "            return self.transform(self.data[index]), None\n",
    "        else:\n",
    "            return self.transform(self.data[index]), \\\n",
    "                   self.target_transform(self.label[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def prepare(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = x / 2.0\n",
    "    return y\n",
    "\n",
    "train_set = dezero.datasets.Spiral(transform=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import transforms\n",
    "\n",
    "f = transforms.Normalize(mean=0.0, std=2.0)\n",
    "train_set = dezero.datasets.Spiral(transform=f)\n",
    "\n",
    "f = transforms.Compose([transforms.Normalize(mean=0.0, std=2.0),\n",
    "                        transforms.AsType(np.float64)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 50 미니배치를 뽑아주는 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13772/3277225742.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 반복자(iterator)\n",
    "t = [1, 2, 3]\n",
    "x = iter(t)\n",
    "print(next(x))\n",
    "print(next(x))\n",
    "print(next(x))\n",
    "print(next(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator:\n",
    "    def __init__(self, max_cnt):\n",
    "        self.max_cnt = max_cnt\n",
    "        self.cnt = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.cnt == self.max_cnt:\n",
    "            raise StopIteration()\n",
    "        \n",
    "        self.cnt += 1\n",
    "        return self.cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "obj = MyIterator(5)\n",
    "for x in obj:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_size = len(dataset)\n",
    "        self.max_iter = math.ceil(self.data_size / batch_size)\n",
    "\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.iteration = 0  # 반복 횟수 초기화\n",
    "        if self.shuffle:\n",
    "            self.index = np.random.permutation(len(self.dataset))\n",
    "        else:\n",
    "            self.index = np.arange(len(self.dataset))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.iteration >= self.max_iter:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "    \n",
    "        i, batch_size = self.iteration, self.batch_size\n",
    "        batch_index = self.index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch = [self.dataset[i] for i in batch_index]\n",
    "        x = np.array([example[0] for example in batch])\n",
    "        t = np.array([example[1] for example in batch])\n",
    "\n",
    "        self.iteration += 1\n",
    "        return x, t\n",
    "    \n",
    "    def next(self):\n",
    "        return self.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) (10,)\n",
      "(10, 2) (10,)\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 사용\n",
    "from dezero.datasets import Spiral\n",
    "from dezero import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "max_epoch = 1\n",
    "\n",
    "train_set = Spiral(train=True)\n",
    "test_set = Spiral(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for x, t in train_loader:\n",
    "        print(x.shape, t.shape)\n",
    "        break\n",
    "\n",
    "    for x, t in test_loader:\n",
    "        print(x.shape, t.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 함수\n",
    "def accuracy(y, t):\n",
    "    y, t = as_variable(y), as_variable(t)\n",
    "\n",
    "    pred = y.data.argmax(axis=1).reshape(t.shape)\n",
    "    result = (pred == t.data)\n",
    "    acc = result.mean()\n",
    "    return Variable(as_array(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.6666666666666666)\n"
     ]
    }
   ],
   "source": [
    "# accuracy 함수 활용\n",
    "y = np.array([[0.2, 0.8, 0], [0.1, 0.9, 0], [0.8, 0.1, 0.1]])\n",
    "t = np.array([1, 2, 0])\n",
    "acc = F.accuracy(y, t)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 1.0944, accuracy: 0.4033\n",
      "test loss: 1.0468, accuracy: 0.3267\n",
      "epoch: 2\n",
      "train loss: 0.9882, accuracy: 0.4933\n",
      "test loss: 0.9729, accuracy: 0.4333\n",
      "epoch: 3\n",
      "train loss: 0.9403, accuracy: 0.5133\n",
      "test loss: 0.8965, accuracy: 0.6233\n",
      "epoch: 4\n",
      "train loss: 0.8820, accuracy: 0.5700\n",
      "test loss: 0.8771, accuracy: 0.5967\n",
      "epoch: 5\n",
      "train loss: 0.8617, accuracy: 0.5600\n",
      "test loss: 0.8670, accuracy: 0.5867\n",
      "epoch: 6\n",
      "train loss: 0.8313, accuracy: 0.5300\n",
      "test loss: 0.8654, accuracy: 0.6000\n",
      "epoch: 7\n",
      "train loss: 0.8086, accuracy: 0.5833\n",
      "test loss: 0.7950, accuracy: 0.5600\n",
      "epoch: 8\n",
      "train loss: 0.7948, accuracy: 0.5733\n",
      "test loss: 0.7921, accuracy: 0.5900\n",
      "epoch: 9\n",
      "train loss: 0.7728, accuracy: 0.5500\n",
      "test loss: 0.7718, accuracy: 0.5300\n",
      "epoch: 10\n",
      "train loss: 0.7643, accuracy: 0.5633\n",
      "test loss: 0.7796, accuracy: 0.5800\n",
      "epoch: 11\n",
      "train loss: 0.7862, accuracy: 0.5600\n",
      "test loss: 0.7701, accuracy: 0.5633\n",
      "epoch: 12\n",
      "train loss: 0.7914, accuracy: 0.5500\n",
      "test loss: 0.8218, accuracy: 0.6067\n",
      "epoch: 13\n",
      "train loss: 0.7633, accuracy: 0.5567\n",
      "test loss: 0.7757, accuracy: 0.5800\n",
      "epoch: 14\n",
      "train loss: 0.7612, accuracy: 0.5833\n",
      "test loss: 0.7800, accuracy: 0.5667\n",
      "epoch: 15\n",
      "train loss: 0.7408, accuracy: 0.5667\n",
      "test loss: 0.7654, accuracy: 0.5867\n",
      "epoch: 16\n",
      "train loss: 0.7453, accuracy: 0.5733\n",
      "test loss: 0.7648, accuracy: 0.6033\n",
      "epoch: 17\n",
      "train loss: 0.7843, accuracy: 0.5667\n",
      "test loss: 0.7415, accuracy: 0.5267\n",
      "epoch: 18\n",
      "train loss: 0.7592, accuracy: 0.5633\n",
      "test loss: 0.7388, accuracy: 0.5267\n",
      "epoch: 19\n",
      "train loss: 0.7396, accuracy: 0.5567\n",
      "test loss: 0.7791, accuracy: 0.5667\n",
      "epoch: 20\n",
      "train loss: 0.7376, accuracy: 0.5533\n",
      "test loss: 0.7478, accuracy: 0.5200\n",
      "epoch: 21\n",
      "train loss: 0.7469, accuracy: 0.5600\n",
      "test loss: 0.7617, accuracy: 0.6100\n",
      "epoch: 22\n",
      "train loss: 0.7349, accuracy: 0.5667\n",
      "test loss: 0.7476, accuracy: 0.5967\n",
      "epoch: 23\n",
      "train loss: 0.7650, accuracy: 0.5600\n",
      "test loss: 0.7336, accuracy: 0.5433\n",
      "epoch: 24\n",
      "train loss: 0.7495, accuracy: 0.5667\n",
      "test loss: 0.7366, accuracy: 0.5500\n",
      "epoch: 25\n",
      "train loss: 0.7414, accuracy: 0.5700\n",
      "test loss: 0.7516, accuracy: 0.6133\n",
      "epoch: 26\n",
      "train loss: 0.7298, accuracy: 0.5900\n",
      "test loss: 0.7327, accuracy: 0.5533\n",
      "epoch: 27\n",
      "train loss: 0.7297, accuracy: 0.5700\n",
      "test loss: 0.7356, accuracy: 0.5933\n",
      "epoch: 28\n",
      "train loss: 0.7267, accuracy: 0.5900\n",
      "test loss: 0.7240, accuracy: 0.5233\n",
      "epoch: 29\n",
      "train loss: 0.7359, accuracy: 0.5800\n",
      "test loss: 0.7203, accuracy: 0.5533\n",
      "epoch: 30\n",
      "train loss: 0.7273, accuracy: 0.5600\n",
      "test loss: 0.7382, accuracy: 0.5433\n",
      "epoch: 31\n",
      "train loss: 0.7338, accuracy: 0.5867\n",
      "test loss: 0.7492, accuracy: 0.5933\n",
      "epoch: 32\n",
      "train loss: 0.7092, accuracy: 0.5900\n",
      "test loss: 0.7241, accuracy: 0.5500\n",
      "epoch: 33\n",
      "train loss: 0.6970, accuracy: 0.5767\n",
      "test loss: 0.7483, accuracy: 0.6167\n",
      "epoch: 34\n",
      "train loss: 0.7048, accuracy: 0.5967\n",
      "test loss: 0.7170, accuracy: 0.5433\n",
      "epoch: 35\n",
      "train loss: 0.7069, accuracy: 0.5967\n",
      "test loss: 0.7164, accuracy: 0.5467\n",
      "epoch: 36\n",
      "train loss: 0.7199, accuracy: 0.5800\n",
      "test loss: 0.7318, accuracy: 0.6133\n",
      "epoch: 37\n",
      "train loss: 0.7035, accuracy: 0.6167\n",
      "test loss: 0.7266, accuracy: 0.5900\n",
      "epoch: 38\n",
      "train loss: 0.6992, accuracy: 0.5767\n",
      "test loss: 0.7419, accuracy: 0.6000\n",
      "epoch: 39\n",
      "train loss: 0.7103, accuracy: 0.6000\n",
      "test loss: 0.7077, accuracy: 0.5333\n",
      "epoch: 40\n",
      "train loss: 0.7064, accuracy: 0.5733\n",
      "test loss: 0.7335, accuracy: 0.5900\n",
      "epoch: 41\n",
      "train loss: 0.7022, accuracy: 0.5767\n",
      "test loss: 0.7087, accuracy: 0.6100\n",
      "epoch: 42\n",
      "train loss: 0.7047, accuracy: 0.6000\n",
      "test loss: 0.6951, accuracy: 0.5800\n",
      "epoch: 43\n",
      "train loss: 0.7071, accuracy: 0.6000\n",
      "test loss: 0.7009, accuracy: 0.6133\n",
      "epoch: 44\n",
      "train loss: 0.7041, accuracy: 0.6033\n",
      "test loss: 0.6914, accuracy: 0.5833\n",
      "epoch: 45\n",
      "train loss: 0.6854, accuracy: 0.5767\n",
      "test loss: 0.7122, accuracy: 0.6100\n",
      "epoch: 46\n",
      "train loss: 0.6771, accuracy: 0.6067\n",
      "test loss: 0.6874, accuracy: 0.5600\n",
      "epoch: 47\n",
      "train loss: 0.6781, accuracy: 0.5900\n",
      "test loss: 0.6995, accuracy: 0.6467\n",
      "epoch: 48\n",
      "train loss: 0.6875, accuracy: 0.5933\n",
      "test loss: 0.7021, accuracy: 0.6300\n",
      "epoch: 49\n",
      "train loss: 0.6641, accuracy: 0.6000\n",
      "test loss: 0.6897, accuracy: 0.6000\n",
      "epoch: 50\n",
      "train loss: 0.6683, accuracy: 0.6200\n",
      "test loss: 0.6850, accuracy: 0.5867\n",
      "epoch: 51\n",
      "train loss: 0.6554, accuracy: 0.6033\n",
      "test loss: 0.6698, accuracy: 0.6367\n",
      "epoch: 52\n",
      "train loss: 0.6528, accuracy: 0.6200\n",
      "test loss: 0.6789, accuracy: 0.5800\n",
      "epoch: 53\n",
      "train loss: 0.6500, accuracy: 0.6467\n",
      "test loss: 0.6691, accuracy: 0.6467\n",
      "epoch: 54\n",
      "train loss: 0.6449, accuracy: 0.6267\n",
      "test loss: 0.6562, accuracy: 0.6633\n",
      "epoch: 55\n",
      "train loss: 0.6549, accuracy: 0.6500\n",
      "test loss: 0.6892, accuracy: 0.6467\n",
      "epoch: 56\n",
      "train loss: 0.6504, accuracy: 0.6400\n",
      "test loss: 0.6421, accuracy: 0.6167\n",
      "epoch: 57\n",
      "train loss: 0.6396, accuracy: 0.6667\n",
      "test loss: 0.6451, accuracy: 0.6167\n",
      "epoch: 58\n",
      "train loss: 0.6251, accuracy: 0.6733\n",
      "test loss: 0.6415, accuracy: 0.6200\n",
      "epoch: 59\n",
      "train loss: 0.6150, accuracy: 0.6433\n",
      "test loss: 0.6285, accuracy: 0.6267\n",
      "epoch: 60\n",
      "train loss: 0.6122, accuracy: 0.6633\n",
      "test loss: 0.6169, accuracy: 0.6400\n",
      "epoch: 61\n",
      "train loss: 0.5987, accuracy: 0.6767\n",
      "test loss: 0.6183, accuracy: 0.6167\n",
      "epoch: 62\n",
      "train loss: 0.5920, accuracy: 0.6667\n",
      "test loss: 0.6076, accuracy: 0.6967\n",
      "epoch: 63\n",
      "train loss: 0.5914, accuracy: 0.6700\n",
      "test loss: 0.6020, accuracy: 0.6300\n",
      "epoch: 64\n",
      "train loss: 0.5911, accuracy: 0.6867\n",
      "test loss: 0.5981, accuracy: 0.6533\n",
      "epoch: 65\n",
      "train loss: 0.5626, accuracy: 0.7000\n",
      "test loss: 0.6124, accuracy: 0.7100\n",
      "epoch: 66\n",
      "train loss: 0.5707, accuracy: 0.7067\n",
      "test loss: 0.5760, accuracy: 0.6567\n",
      "epoch: 67\n",
      "train loss: 0.5574, accuracy: 0.6900\n",
      "test loss: 0.5834, accuracy: 0.6967\n",
      "epoch: 68\n",
      "train loss: 0.5560, accuracy: 0.7133\n",
      "test loss: 0.5589, accuracy: 0.6933\n",
      "epoch: 69\n",
      "train loss: 0.5355, accuracy: 0.7267\n",
      "test loss: 0.5530, accuracy: 0.6967\n",
      "epoch: 70\n",
      "train loss: 0.5303, accuracy: 0.7300\n",
      "test loss: 0.5380, accuracy: 0.7267\n",
      "epoch: 71\n",
      "train loss: 0.5207, accuracy: 0.7167\n",
      "test loss: 0.5315, accuracy: 0.7367\n",
      "epoch: 72\n",
      "train loss: 0.5094, accuracy: 0.7533\n",
      "test loss: 0.5340, accuracy: 0.7367\n",
      "epoch: 73\n",
      "train loss: 0.5110, accuracy: 0.7400\n",
      "test loss: 0.5127, accuracy: 0.7433\n",
      "epoch: 74\n",
      "train loss: 0.4921, accuracy: 0.7500\n",
      "test loss: 0.5051, accuracy: 0.7400\n",
      "epoch: 75\n",
      "train loss: 0.4882, accuracy: 0.7700\n",
      "test loss: 0.5268, accuracy: 0.7467\n",
      "epoch: 76\n",
      "train loss: 0.4859, accuracy: 0.7533\n",
      "test loss: 0.4997, accuracy: 0.7400\n",
      "epoch: 77\n",
      "train loss: 0.4701, accuracy: 0.7767\n",
      "test loss: 0.4814, accuracy: 0.7800\n",
      "epoch: 78\n",
      "train loss: 0.4574, accuracy: 0.7733\n",
      "test loss: 0.4736, accuracy: 0.7700\n",
      "epoch: 79\n",
      "train loss: 0.4547, accuracy: 0.7533\n",
      "test loss: 0.4665, accuracy: 0.7900\n",
      "epoch: 80\n",
      "train loss: 0.4533, accuracy: 0.7667\n",
      "test loss: 0.4601, accuracy: 0.7900\n",
      "epoch: 81\n",
      "train loss: 0.4431, accuracy: 0.7967\n",
      "test loss: 0.4496, accuracy: 0.7900\n",
      "epoch: 82\n",
      "train loss: 0.4309, accuracy: 0.8167\n",
      "test loss: 0.4421, accuracy: 0.7867\n",
      "epoch: 83\n",
      "train loss: 0.4282, accuracy: 0.8167\n",
      "test loss: 0.4431, accuracy: 0.7833\n",
      "epoch: 84\n",
      "train loss: 0.4132, accuracy: 0.8167\n",
      "test loss: 0.4353, accuracy: 0.8200\n",
      "epoch: 85\n",
      "train loss: 0.4132, accuracy: 0.8167\n",
      "test loss: 0.4260, accuracy: 0.8367\n",
      "epoch: 86\n",
      "train loss: 0.4047, accuracy: 0.8167\n",
      "test loss: 0.4248, accuracy: 0.8400\n",
      "epoch: 87\n",
      "train loss: 0.3968, accuracy: 0.8367\n",
      "test loss: 0.4031, accuracy: 0.8333\n",
      "epoch: 88\n",
      "train loss: 0.3942, accuracy: 0.8267\n",
      "test loss: 0.4091, accuracy: 0.8333\n",
      "epoch: 89\n",
      "train loss: 0.3746, accuracy: 0.8333\n",
      "test loss: 0.3962, accuracy: 0.8100\n",
      "epoch: 90\n",
      "train loss: 0.3772, accuracy: 0.8333\n",
      "test loss: 0.3831, accuracy: 0.8300\n",
      "epoch: 91\n",
      "train loss: 0.3669, accuracy: 0.8467\n",
      "test loss: 0.3844, accuracy: 0.8433\n",
      "epoch: 92\n",
      "train loss: 0.3554, accuracy: 0.8600\n",
      "test loss: 0.3752, accuracy: 0.8567\n",
      "epoch: 93\n",
      "train loss: 0.3491, accuracy: 0.8700\n",
      "test loss: 0.3720, accuracy: 0.8400\n",
      "epoch: 94\n",
      "train loss: 0.3440, accuracy: 0.8533\n",
      "test loss: 0.3595, accuracy: 0.8600\n",
      "epoch: 95\n",
      "train loss: 0.3408, accuracy: 0.8633\n",
      "test loss: 0.3533, accuracy: 0.8700\n",
      "epoch: 96\n",
      "train loss: 0.3336, accuracy: 0.8600\n",
      "test loss: 0.3515, accuracy: 0.8800\n",
      "epoch: 97\n",
      "train loss: 0.3295, accuracy: 0.8700\n",
      "test loss: 0.3437, accuracy: 0.8933\n",
      "epoch: 98\n",
      "train loss: 0.3208, accuracy: 0.9133\n",
      "test loss: 0.3433, accuracy: 0.8533\n",
      "epoch: 99\n",
      "train loss: 0.3212, accuracy: 0.8667\n",
      "test loss: 0.3302, accuracy: 0.8633\n",
      "epoch: 100\n",
      "train loss: 0.3146, accuracy: 0.8700\n",
      "test loss: 0.3248, accuracy: 0.8600\n",
      "epoch: 101\n",
      "train loss: 0.3090, accuracy: 0.8633\n",
      "test loss: 0.3268, accuracy: 0.8833\n",
      "epoch: 102\n",
      "train loss: 0.3024, accuracy: 0.8933\n",
      "test loss: 0.3195, accuracy: 0.8533\n",
      "epoch: 103\n",
      "train loss: 0.3009, accuracy: 0.8933\n",
      "test loss: 0.3122, accuracy: 0.8900\n",
      "epoch: 104\n",
      "train loss: 0.2930, accuracy: 0.9067\n",
      "test loss: 0.3087, accuracy: 0.8633\n",
      "epoch: 105\n",
      "train loss: 0.2881, accuracy: 0.9067\n",
      "test loss: 0.3111, accuracy: 0.8567\n",
      "epoch: 106\n",
      "train loss: 0.2785, accuracy: 0.9167\n",
      "test loss: 0.3026, accuracy: 0.8867\n",
      "epoch: 107\n",
      "train loss: 0.2786, accuracy: 0.8933\n",
      "test loss: 0.2952, accuracy: 0.8833\n",
      "epoch: 108\n",
      "train loss: 0.2756, accuracy: 0.9100\n",
      "test loss: 0.2912, accuracy: 0.8800\n",
      "epoch: 109\n",
      "train loss: 0.2744, accuracy: 0.9200\n",
      "test loss: 0.2965, accuracy: 0.8667\n",
      "epoch: 110\n",
      "train loss: 0.2705, accuracy: 0.9267\n",
      "test loss: 0.2856, accuracy: 0.8933\n",
      "epoch: 111\n",
      "train loss: 0.2645, accuracy: 0.9067\n",
      "test loss: 0.2814, accuracy: 0.8867\n",
      "epoch: 112\n",
      "train loss: 0.2624, accuracy: 0.9100\n",
      "test loss: 0.2835, accuracy: 0.8700\n",
      "epoch: 113\n",
      "train loss: 0.2608, accuracy: 0.9067\n",
      "test loss: 0.2752, accuracy: 0.8933\n",
      "epoch: 114\n",
      "train loss: 0.2524, accuracy: 0.9167\n",
      "test loss: 0.2740, accuracy: 0.9033\n",
      "epoch: 115\n",
      "train loss: 0.2484, accuracy: 0.9167\n",
      "test loss: 0.2688, accuracy: 0.8967\n",
      "epoch: 116\n",
      "train loss: 0.2480, accuracy: 0.9200\n",
      "test loss: 0.2768, accuracy: 0.8600\n",
      "epoch: 117\n",
      "train loss: 0.2445, accuracy: 0.9133\n",
      "test loss: 0.2635, accuracy: 0.9067\n",
      "epoch: 118\n",
      "train loss: 0.2444, accuracy: 0.9267\n",
      "test loss: 0.2593, accuracy: 0.8967\n",
      "epoch: 119\n",
      "train loss: 0.2379, accuracy: 0.9267\n",
      "test loss: 0.2576, accuracy: 0.8867\n",
      "epoch: 120\n",
      "train loss: 0.2361, accuracy: 0.9200\n",
      "test loss: 0.2573, accuracy: 0.9200\n",
      "epoch: 121\n",
      "train loss: 0.2372, accuracy: 0.9333\n",
      "test loss: 0.2534, accuracy: 0.9067\n",
      "epoch: 122\n",
      "train loss: 0.2305, accuracy: 0.9333\n",
      "test loss: 0.2511, accuracy: 0.8867\n",
      "epoch: 123\n",
      "train loss: 0.2277, accuracy: 0.9367\n",
      "test loss: 0.2531, accuracy: 0.9000\n",
      "epoch: 124\n",
      "train loss: 0.2318, accuracy: 0.9233\n",
      "test loss: 0.2456, accuracy: 0.9067\n",
      "epoch: 125\n",
      "train loss: 0.2248, accuracy: 0.9400\n",
      "test loss: 0.2454, accuracy: 0.9133\n",
      "epoch: 126\n",
      "train loss: 0.2229, accuracy: 0.9433\n",
      "test loss: 0.2427, accuracy: 0.9033\n",
      "epoch: 127\n",
      "train loss: 0.2264, accuracy: 0.9167\n",
      "test loss: 0.2433, accuracy: 0.9000\n",
      "epoch: 128\n",
      "train loss: 0.2152, accuracy: 0.9333\n",
      "test loss: 0.2373, accuracy: 0.9133\n",
      "epoch: 129\n",
      "train loss: 0.2176, accuracy: 0.9367\n",
      "test loss: 0.2402, accuracy: 0.8967\n",
      "epoch: 130\n",
      "train loss: 0.2149, accuracy: 0.9200\n",
      "test loss: 0.2332, accuracy: 0.9033\n",
      "epoch: 131\n",
      "train loss: 0.2136, accuracy: 0.9333\n",
      "test loss: 0.2377, accuracy: 0.8933\n",
      "epoch: 132\n",
      "train loss: 0.2143, accuracy: 0.9200\n",
      "test loss: 0.2304, accuracy: 0.9167\n",
      "epoch: 133\n",
      "train loss: 0.2141, accuracy: 0.9267\n",
      "test loss: 0.2449, accuracy: 0.8833\n",
      "epoch: 134\n",
      "train loss: 0.2088, accuracy: 0.9133\n",
      "test loss: 0.2362, accuracy: 0.8900\n",
      "epoch: 135\n",
      "train loss: 0.2046, accuracy: 0.9333\n",
      "test loss: 0.2278, accuracy: 0.9133\n",
      "epoch: 136\n",
      "train loss: 0.2021, accuracy: 0.9333\n",
      "test loss: 0.2275, accuracy: 0.9200\n",
      "epoch: 137\n",
      "train loss: 0.2046, accuracy: 0.9333\n",
      "test loss: 0.2236, accuracy: 0.9167\n",
      "epoch: 138\n",
      "train loss: 0.1991, accuracy: 0.9267\n",
      "test loss: 0.2207, accuracy: 0.9233\n",
      "epoch: 139\n",
      "train loss: 0.1960, accuracy: 0.9433\n",
      "test loss: 0.2216, accuracy: 0.9267\n",
      "epoch: 140\n",
      "train loss: 0.1981, accuracy: 0.9333\n",
      "test loss: 0.2178, accuracy: 0.9233\n",
      "epoch: 141\n",
      "train loss: 0.1945, accuracy: 0.9367\n",
      "test loss: 0.2194, accuracy: 0.9200\n",
      "epoch: 142\n",
      "train loss: 0.1969, accuracy: 0.9233\n",
      "test loss: 0.2158, accuracy: 0.9233\n",
      "epoch: 143\n",
      "train loss: 0.1942, accuracy: 0.9333\n",
      "test loss: 0.2127, accuracy: 0.9167\n",
      "epoch: 144\n",
      "train loss: 0.1886, accuracy: 0.9433\n",
      "test loss: 0.2288, accuracy: 0.8933\n",
      "epoch: 145\n",
      "train loss: 0.1966, accuracy: 0.9133\n",
      "test loss: 0.2103, accuracy: 0.9167\n",
      "epoch: 146\n",
      "train loss: 0.1899, accuracy: 0.9300\n",
      "test loss: 0.2124, accuracy: 0.9100\n",
      "epoch: 147\n",
      "train loss: 0.1890, accuracy: 0.9400\n",
      "test loss: 0.2104, accuracy: 0.9200\n",
      "epoch: 148\n",
      "train loss: 0.1865, accuracy: 0.9333\n",
      "test loss: 0.2080, accuracy: 0.9267\n",
      "epoch: 149\n",
      "train loss: 0.1825, accuracy: 0.9367\n",
      "test loss: 0.2285, accuracy: 0.8967\n",
      "epoch: 150\n",
      "train loss: 0.1890, accuracy: 0.9233\n",
      "test loss: 0.2152, accuracy: 0.9067\n",
      "epoch: 151\n",
      "train loss: 0.1828, accuracy: 0.9400\n",
      "test loss: 0.2042, accuracy: 0.9300\n",
      "epoch: 152\n",
      "train loss: 0.1807, accuracy: 0.9467\n",
      "test loss: 0.2136, accuracy: 0.9067\n",
      "epoch: 153\n",
      "train loss: 0.1795, accuracy: 0.9333\n",
      "test loss: 0.2076, accuracy: 0.9033\n",
      "epoch: 154\n",
      "train loss: 0.1797, accuracy: 0.9300\n",
      "test loss: 0.2052, accuracy: 0.9200\n",
      "epoch: 155\n",
      "train loss: 0.1816, accuracy: 0.9400\n",
      "test loss: 0.2015, accuracy: 0.9233\n",
      "epoch: 156\n",
      "train loss: 0.1791, accuracy: 0.9300\n",
      "test loss: 0.2012, accuracy: 0.9333\n",
      "epoch: 157\n",
      "train loss: 0.1791, accuracy: 0.9367\n",
      "test loss: 0.1978, accuracy: 0.9267\n",
      "epoch: 158\n",
      "train loss: 0.1751, accuracy: 0.9400\n",
      "test loss: 0.2046, accuracy: 0.9033\n",
      "epoch: 159\n",
      "train loss: 0.1719, accuracy: 0.9433\n",
      "test loss: 0.2039, accuracy: 0.9067\n",
      "epoch: 160\n",
      "train loss: 0.1730, accuracy: 0.9433\n",
      "test loss: 0.1963, accuracy: 0.9300\n",
      "epoch: 161\n",
      "train loss: 0.1690, accuracy: 0.9467\n",
      "test loss: 0.1973, accuracy: 0.9267\n",
      "epoch: 162\n",
      "train loss: 0.1676, accuracy: 0.9567\n",
      "test loss: 0.1951, accuracy: 0.9400\n",
      "epoch: 163\n",
      "train loss: 0.1734, accuracy: 0.9433\n",
      "test loss: 0.1941, accuracy: 0.9300\n",
      "epoch: 164\n",
      "train loss: 0.1725, accuracy: 0.9233\n",
      "test loss: 0.1922, accuracy: 0.9367\n",
      "epoch: 165\n",
      "train loss: 0.1665, accuracy: 0.9433\n",
      "test loss: 0.1951, accuracy: 0.9233\n",
      "epoch: 166\n",
      "train loss: 0.1698, accuracy: 0.9433\n",
      "test loss: 0.1907, accuracy: 0.9333\n",
      "epoch: 167\n",
      "train loss: 0.1737, accuracy: 0.9300\n",
      "test loss: 0.1923, accuracy: 0.9300\n",
      "epoch: 168\n",
      "train loss: 0.1661, accuracy: 0.9367\n",
      "test loss: 0.1944, accuracy: 0.9300\n",
      "epoch: 169\n",
      "train loss: 0.1635, accuracy: 0.9567\n",
      "test loss: 0.1894, accuracy: 0.9233\n",
      "epoch: 170\n",
      "train loss: 0.1688, accuracy: 0.9400\n",
      "test loss: 0.1878, accuracy: 0.9300\n",
      "epoch: 171\n",
      "train loss: 0.1675, accuracy: 0.9300\n",
      "test loss: 0.1899, accuracy: 0.9367\n",
      "epoch: 172\n",
      "train loss: 0.1602, accuracy: 0.9500\n",
      "test loss: 0.1859, accuracy: 0.9400\n",
      "epoch: 173\n",
      "train loss: 0.1644, accuracy: 0.9433\n",
      "test loss: 0.1901, accuracy: 0.9267\n",
      "epoch: 174\n",
      "train loss: 0.1597, accuracy: 0.9500\n",
      "test loss: 0.1878, accuracy: 0.9267\n",
      "epoch: 175\n",
      "train loss: 0.1599, accuracy: 0.9400\n",
      "test loss: 0.1840, accuracy: 0.9367\n",
      "epoch: 176\n",
      "train loss: 0.1612, accuracy: 0.9533\n",
      "test loss: 0.1909, accuracy: 0.9200\n",
      "epoch: 177\n",
      "train loss: 0.1582, accuracy: 0.9500\n",
      "test loss: 0.1829, accuracy: 0.9433\n",
      "epoch: 178\n",
      "train loss: 0.1599, accuracy: 0.9433\n",
      "test loss: 0.1847, accuracy: 0.9333\n",
      "epoch: 179\n",
      "train loss: 0.1541, accuracy: 0.9400\n",
      "test loss: 0.1892, accuracy: 0.9267\n",
      "epoch: 180\n",
      "train loss: 0.1539, accuracy: 0.9500\n",
      "test loss: 0.1814, accuracy: 0.9367\n",
      "epoch: 181\n",
      "train loss: 0.1572, accuracy: 0.9400\n",
      "test loss: 0.1960, accuracy: 0.9333\n",
      "epoch: 182\n",
      "train loss: 0.1612, accuracy: 0.9400\n",
      "test loss: 0.1824, accuracy: 0.9367\n",
      "epoch: 183\n",
      "train loss: 0.1533, accuracy: 0.9533\n",
      "test loss: 0.1836, accuracy: 0.9233\n",
      "epoch: 184\n",
      "train loss: 0.1526, accuracy: 0.9533\n",
      "test loss: 0.1787, accuracy: 0.9433\n",
      "epoch: 185\n",
      "train loss: 0.1530, accuracy: 0.9533\n",
      "test loss: 0.1892, accuracy: 0.9100\n",
      "epoch: 186\n",
      "train loss: 0.1558, accuracy: 0.9433\n",
      "test loss: 0.1840, accuracy: 0.9267\n",
      "epoch: 187\n",
      "train loss: 0.1559, accuracy: 0.9400\n",
      "test loss: 0.1775, accuracy: 0.9367\n",
      "epoch: 188\n",
      "train loss: 0.1513, accuracy: 0.9500\n",
      "test loss: 0.1775, accuracy: 0.9367\n",
      "epoch: 189\n",
      "train loss: 0.1491, accuracy: 0.9533\n",
      "test loss: 0.1850, accuracy: 0.9267\n",
      "epoch: 190\n",
      "train loss: 0.1488, accuracy: 0.9500\n",
      "test loss: 0.1790, accuracy: 0.9333\n",
      "epoch: 191\n",
      "train loss: 0.1499, accuracy: 0.9467\n",
      "test loss: 0.1755, accuracy: 0.9400\n",
      "epoch: 192\n",
      "train loss: 0.1469, accuracy: 0.9433\n",
      "test loss: 0.1788, accuracy: 0.9267\n",
      "epoch: 193\n",
      "train loss: 0.1528, accuracy: 0.9433\n",
      "test loss: 0.1769, accuracy: 0.9367\n",
      "epoch: 194\n",
      "train loss: 0.1482, accuracy: 0.9433\n",
      "test loss: 0.1737, accuracy: 0.9467\n",
      "epoch: 195\n",
      "train loss: 0.1425, accuracy: 0.9533\n",
      "test loss: 0.1770, accuracy: 0.9333\n",
      "epoch: 196\n",
      "train loss: 0.1493, accuracy: 0.9433\n",
      "test loss: 0.1757, accuracy: 0.9300\n",
      "epoch: 197\n",
      "train loss: 0.1436, accuracy: 0.9467\n",
      "test loss: 0.1752, accuracy: 0.9300\n",
      "epoch: 198\n",
      "train loss: 0.1449, accuracy: 0.9433\n",
      "test loss: 0.1721, accuracy: 0.9433\n",
      "epoch: 199\n",
      "train loss: 0.1456, accuracy: 0.9467\n",
      "test loss: 0.1734, accuracy: 0.9300\n",
      "epoch: 200\n",
      "train loss: 0.1447, accuracy: 0.9500\n",
      "test loss: 0.1710, accuracy: 0.9433\n",
      "epoch: 201\n",
      "train loss: 0.1426, accuracy: 0.9500\n",
      "test loss: 0.1707, accuracy: 0.9433\n",
      "epoch: 202\n",
      "train loss: 0.1406, accuracy: 0.9600\n",
      "test loss: 0.1733, accuracy: 0.9400\n",
      "epoch: 203\n",
      "train loss: 0.1388, accuracy: 0.9667\n",
      "test loss: 0.1760, accuracy: 0.9333\n",
      "epoch: 204\n",
      "train loss: 0.1427, accuracy: 0.9433\n",
      "test loss: 0.1696, accuracy: 0.9467\n",
      "epoch: 205\n",
      "train loss: 0.1403, accuracy: 0.9567\n",
      "test loss: 0.1754, accuracy: 0.9433\n",
      "epoch: 206\n",
      "train loss: 0.1436, accuracy: 0.9400\n",
      "test loss: 0.1698, accuracy: 0.9433\n",
      "epoch: 207\n",
      "train loss: 0.1410, accuracy: 0.9567\n",
      "test loss: 0.1723, accuracy: 0.9333\n",
      "epoch: 208\n",
      "train loss: 0.1418, accuracy: 0.9567\n",
      "test loss: 0.1685, accuracy: 0.9400\n",
      "epoch: 209\n",
      "train loss: 0.1408, accuracy: 0.9467\n",
      "test loss: 0.1695, accuracy: 0.9367\n",
      "epoch: 210\n",
      "train loss: 0.1382, accuracy: 0.9533\n",
      "test loss: 0.1676, accuracy: 0.9467\n",
      "epoch: 211\n",
      "train loss: 0.1359, accuracy: 0.9633\n",
      "test loss: 0.1815, accuracy: 0.9367\n",
      "epoch: 212\n",
      "train loss: 0.1377, accuracy: 0.9600\n",
      "test loss: 0.1677, accuracy: 0.9433\n",
      "epoch: 213\n",
      "train loss: 0.1350, accuracy: 0.9467\n",
      "test loss: 0.1669, accuracy: 0.9467\n",
      "epoch: 214\n",
      "train loss: 0.1340, accuracy: 0.9600\n",
      "test loss: 0.1727, accuracy: 0.9367\n",
      "epoch: 215\n",
      "train loss: 0.1425, accuracy: 0.9567\n",
      "test loss: 0.1654, accuracy: 0.9467\n",
      "epoch: 216\n",
      "train loss: 0.1404, accuracy: 0.9533\n",
      "test loss: 0.1685, accuracy: 0.9333\n",
      "epoch: 217\n",
      "train loss: 0.1349, accuracy: 0.9567\n",
      "test loss: 0.1688, accuracy: 0.9367\n",
      "epoch: 218\n",
      "train loss: 0.1410, accuracy: 0.9533\n",
      "test loss: 0.1678, accuracy: 0.9300\n",
      "epoch: 219\n",
      "train loss: 0.1342, accuracy: 0.9533\n",
      "test loss: 0.1663, accuracy: 0.9400\n",
      "epoch: 220\n",
      "train loss: 0.1361, accuracy: 0.9500\n",
      "test loss: 0.1649, accuracy: 0.9433\n",
      "epoch: 221\n",
      "train loss: 0.1361, accuracy: 0.9500\n",
      "test loss: 0.1632, accuracy: 0.9533\n",
      "epoch: 222\n",
      "train loss: 0.1318, accuracy: 0.9633\n",
      "test loss: 0.1661, accuracy: 0.9433\n",
      "epoch: 223\n",
      "train loss: 0.1345, accuracy: 0.9567\n",
      "test loss: 0.1629, accuracy: 0.9467\n",
      "epoch: 224\n",
      "train loss: 0.1319, accuracy: 0.9600\n",
      "test loss: 0.1629, accuracy: 0.9533\n",
      "epoch: 225\n",
      "train loss: 0.1334, accuracy: 0.9533\n",
      "test loss: 0.1620, accuracy: 0.9500\n",
      "epoch: 226\n",
      "train loss: 0.1339, accuracy: 0.9633\n",
      "test loss: 0.1675, accuracy: 0.9367\n",
      "epoch: 227\n",
      "train loss: 0.1364, accuracy: 0.9467\n",
      "test loss: 0.1613, accuracy: 0.9467\n",
      "epoch: 228\n",
      "train loss: 0.1319, accuracy: 0.9533\n",
      "test loss: 0.1616, accuracy: 0.9467\n",
      "epoch: 229\n",
      "train loss: 0.1301, accuracy: 0.9600\n",
      "test loss: 0.1640, accuracy: 0.9300\n",
      "epoch: 230\n",
      "train loss: 0.1298, accuracy: 0.9467\n",
      "test loss: 0.1656, accuracy: 0.9333\n",
      "epoch: 231\n",
      "train loss: 0.1289, accuracy: 0.9567\n",
      "test loss: 0.1633, accuracy: 0.9500\n",
      "epoch: 232\n",
      "train loss: 0.1276, accuracy: 0.9600\n",
      "test loss: 0.1632, accuracy: 0.9433\n",
      "epoch: 233\n",
      "train loss: 0.1269, accuracy: 0.9567\n",
      "test loss: 0.1675, accuracy: 0.9333\n",
      "epoch: 234\n",
      "train loss: 0.1311, accuracy: 0.9600\n",
      "test loss: 0.1630, accuracy: 0.9433\n",
      "epoch: 235\n",
      "train loss: 0.1335, accuracy: 0.9500\n",
      "test loss: 0.1602, accuracy: 0.9467\n",
      "epoch: 236\n",
      "train loss: 0.1275, accuracy: 0.9633\n",
      "test loss: 0.1596, accuracy: 0.9500\n",
      "epoch: 237\n",
      "train loss: 0.1227, accuracy: 0.9633\n",
      "test loss: 0.1619, accuracy: 0.9467\n",
      "epoch: 238\n",
      "train loss: 0.1275, accuracy: 0.9600\n",
      "test loss: 0.1588, accuracy: 0.9433\n",
      "epoch: 239\n",
      "train loss: 0.1270, accuracy: 0.9533\n",
      "test loss: 0.1657, accuracy: 0.9367\n",
      "epoch: 240\n",
      "train loss: 0.1251, accuracy: 0.9500\n",
      "test loss: 0.1583, accuracy: 0.9433\n",
      "epoch: 241\n",
      "train loss: 0.1273, accuracy: 0.9533\n",
      "test loss: 0.1589, accuracy: 0.9500\n",
      "epoch: 242\n",
      "train loss: 0.1268, accuracy: 0.9567\n",
      "test loss: 0.1584, accuracy: 0.9500\n",
      "epoch: 243\n",
      "train loss: 0.1242, accuracy: 0.9633\n",
      "test loss: 0.1580, accuracy: 0.9433\n",
      "epoch: 244\n",
      "train loss: 0.1232, accuracy: 0.9633\n",
      "test loss: 0.1642, accuracy: 0.9433\n",
      "epoch: 245\n",
      "train loss: 0.1247, accuracy: 0.9600\n",
      "test loss: 0.1562, accuracy: 0.9500\n",
      "epoch: 246\n",
      "train loss: 0.1273, accuracy: 0.9567\n",
      "test loss: 0.1555, accuracy: 0.9500\n",
      "epoch: 247\n",
      "train loss: 0.1305, accuracy: 0.9500\n",
      "test loss: 0.1552, accuracy: 0.9533\n",
      "epoch: 248\n",
      "train loss: 0.1274, accuracy: 0.9500\n",
      "test loss: 0.1579, accuracy: 0.9467\n",
      "epoch: 249\n",
      "train loss: 0.1282, accuracy: 0.9500\n",
      "test loss: 0.1589, accuracy: 0.9500\n",
      "epoch: 250\n",
      "train loss: 0.1286, accuracy: 0.9600\n",
      "test loss: 0.1548, accuracy: 0.9533\n",
      "epoch: 251\n",
      "train loss: 0.1251, accuracy: 0.9667\n",
      "test loss: 0.1566, accuracy: 0.9400\n",
      "epoch: 252\n",
      "train loss: 0.1213, accuracy: 0.9533\n",
      "test loss: 0.1615, accuracy: 0.9167\n",
      "epoch: 253\n",
      "train loss: 0.1232, accuracy: 0.9633\n",
      "test loss: 0.1588, accuracy: 0.9433\n",
      "epoch: 254\n",
      "train loss: 0.1238, accuracy: 0.9567\n",
      "test loss: 0.1564, accuracy: 0.9400\n",
      "epoch: 255\n",
      "train loss: 0.1212, accuracy: 0.9633\n",
      "test loss: 0.1571, accuracy: 0.9533\n",
      "epoch: 256\n",
      "train loss: 0.1261, accuracy: 0.9600\n",
      "test loss: 0.1546, accuracy: 0.9433\n",
      "epoch: 257\n",
      "train loss: 0.1238, accuracy: 0.9600\n",
      "test loss: 0.1575, accuracy: 0.9533\n",
      "epoch: 258\n",
      "train loss: 0.1224, accuracy: 0.9633\n",
      "test loss: 0.1583, accuracy: 0.9333\n",
      "epoch: 259\n",
      "train loss: 0.1239, accuracy: 0.9533\n",
      "test loss: 0.1534, accuracy: 0.9467\n",
      "epoch: 260\n",
      "train loss: 0.1288, accuracy: 0.9567\n",
      "test loss: 0.1522, accuracy: 0.9500\n",
      "epoch: 261\n",
      "train loss: 0.1245, accuracy: 0.9567\n",
      "test loss: 0.1530, accuracy: 0.9500\n",
      "epoch: 262\n",
      "train loss: 0.1225, accuracy: 0.9600\n",
      "test loss: 0.1549, accuracy: 0.9433\n",
      "epoch: 263\n",
      "train loss: 0.1193, accuracy: 0.9533\n",
      "test loss: 0.1523, accuracy: 0.9533\n",
      "epoch: 264\n",
      "train loss: 0.1177, accuracy: 0.9633\n",
      "test loss: 0.1553, accuracy: 0.9433\n",
      "epoch: 265\n",
      "train loss: 0.1140, accuracy: 0.9633\n",
      "test loss: 0.1566, accuracy: 0.9500\n",
      "epoch: 266\n",
      "train loss: 0.1210, accuracy: 0.9500\n",
      "test loss: 0.1507, accuracy: 0.9533\n",
      "epoch: 267\n",
      "train loss: 0.1224, accuracy: 0.9567\n",
      "test loss: 0.1511, accuracy: 0.9500\n",
      "epoch: 268\n",
      "train loss: 0.1234, accuracy: 0.9500\n",
      "test loss: 0.1504, accuracy: 0.9533\n",
      "epoch: 269\n",
      "train loss: 0.1163, accuracy: 0.9567\n",
      "test loss: 0.1511, accuracy: 0.9500\n",
      "epoch: 270\n",
      "train loss: 0.1191, accuracy: 0.9567\n",
      "test loss: 0.1507, accuracy: 0.9533\n",
      "epoch: 271\n",
      "train loss: 0.1187, accuracy: 0.9567\n",
      "test loss: 0.1543, accuracy: 0.9367\n",
      "epoch: 272\n",
      "train loss: 0.1182, accuracy: 0.9633\n",
      "test loss: 0.1567, accuracy: 0.9267\n",
      "epoch: 273\n",
      "train loss: 0.1137, accuracy: 0.9567\n",
      "test loss: 0.1490, accuracy: 0.9533\n",
      "epoch: 274\n",
      "train loss: 0.1205, accuracy: 0.9467\n",
      "test loss: 0.1541, accuracy: 0.9400\n",
      "epoch: 275\n",
      "train loss: 0.1159, accuracy: 0.9633\n",
      "test loss: 0.1508, accuracy: 0.9467\n",
      "epoch: 276\n",
      "train loss: 0.1151, accuracy: 0.9633\n",
      "test loss: 0.1518, accuracy: 0.9533\n",
      "epoch: 277\n",
      "train loss: 0.1166, accuracy: 0.9633\n",
      "test loss: 0.1578, accuracy: 0.9233\n",
      "epoch: 278\n",
      "train loss: 0.1172, accuracy: 0.9667\n",
      "test loss: 0.1548, accuracy: 0.9400\n",
      "epoch: 279\n",
      "train loss: 0.1134, accuracy: 0.9633\n",
      "test loss: 0.1499, accuracy: 0.9533\n",
      "epoch: 280\n",
      "train loss: 0.1162, accuracy: 0.9533\n",
      "test loss: 0.1506, accuracy: 0.9500\n",
      "epoch: 281\n",
      "train loss: 0.1189, accuracy: 0.9700\n",
      "test loss: 0.1478, accuracy: 0.9567\n",
      "epoch: 282\n",
      "train loss: 0.1188, accuracy: 0.9600\n",
      "test loss: 0.1484, accuracy: 0.9533\n",
      "epoch: 283\n",
      "train loss: 0.1142, accuracy: 0.9667\n",
      "test loss: 0.1524, accuracy: 0.9433\n",
      "epoch: 284\n",
      "train loss: 0.1123, accuracy: 0.9700\n",
      "test loss: 0.1500, accuracy: 0.9433\n",
      "epoch: 285\n",
      "train loss: 0.1192, accuracy: 0.9600\n",
      "test loss: 0.1508, accuracy: 0.9467\n",
      "epoch: 286\n",
      "train loss: 0.1114, accuracy: 0.9600\n",
      "test loss: 0.1471, accuracy: 0.9533\n",
      "epoch: 287\n",
      "train loss: 0.1109, accuracy: 0.9633\n",
      "test loss: 0.1481, accuracy: 0.9567\n",
      "epoch: 288\n",
      "train loss: 0.1143, accuracy: 0.9533\n",
      "test loss: 0.1534, accuracy: 0.9467\n",
      "epoch: 289\n",
      "train loss: 0.1126, accuracy: 0.9600\n",
      "test loss: 0.1486, accuracy: 0.9500\n",
      "epoch: 290\n",
      "train loss: 0.1125, accuracy: 0.9733\n",
      "test loss: 0.1485, accuracy: 0.9533\n",
      "epoch: 291\n",
      "train loss: 0.1136, accuracy: 0.9700\n",
      "test loss: 0.1472, accuracy: 0.9500\n",
      "epoch: 292\n",
      "train loss: 0.1130, accuracy: 0.9600\n",
      "test loss: 0.1495, accuracy: 0.9433\n",
      "epoch: 293\n",
      "train loss: 0.1118, accuracy: 0.9700\n",
      "test loss: 0.1456, accuracy: 0.9533\n",
      "epoch: 294\n",
      "train loss: 0.1111, accuracy: 0.9600\n",
      "test loss: 0.1591, accuracy: 0.9367\n",
      "epoch: 295\n",
      "train loss: 0.1170, accuracy: 0.9533\n",
      "test loss: 0.1451, accuracy: 0.9533\n",
      "epoch: 296\n",
      "train loss: 0.1115, accuracy: 0.9567\n",
      "test loss: 0.1459, accuracy: 0.9567\n",
      "epoch: 297\n",
      "train loss: 0.1096, accuracy: 0.9667\n",
      "test loss: 0.1464, accuracy: 0.9533\n",
      "epoch: 298\n",
      "train loss: 0.1128, accuracy: 0.9500\n",
      "test loss: 0.1476, accuracy: 0.9533\n",
      "epoch: 299\n",
      "train loss: 0.1090, accuracy: 0.9633\n",
      "test loss: 0.1471, accuracy: 0.9467\n",
      "epoch: 300\n",
      "train loss: 0.1082, accuracy: 0.9667\n",
      "test loss: 0.1458, accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "test_set = dezero.datasets.Spiral(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(sum_loss / len(train_set),\n",
    "        sum_acc / len(train_set)))\n",
    "    \n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "        \n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 51 MNIST 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: train-images-idx3-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "Downloading: train-labels-idx1-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "Downloading: t10k-images-idx3-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "Downloading: t10k-labels-idx1-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "train_set = dezero.datasets.MNIST(train=True, transform=None)\n",
    "test_set = dezero.datasets.MNIST(train=False, transform=None)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1, 28, 28)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "x, t = train_set[0]\n",
    "print(type(x), x.shape)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print('label:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "def f(x):\n",
    "    x = x.flatten()\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0\n",
    "    return x\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True, transform=f)\n",
    "test_set = dezero.datasets.MNIST(train=False, transform=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 1.9158, accuracy: 0.5507\n",
      "test loss: 1.5420, accuracy: 0.7191\n",
      "epoch: 2\n",
      "train loss: 1.2851, accuracy: 0.7709\n",
      "test loss: 1.0428, accuracy: 0.8168\n",
      "epoch: 3\n",
      "train loss: 0.9253, accuracy: 0.8191\n",
      "test loss: 0.7925, accuracy: 0.8351\n",
      "epoch: 4\n",
      "train loss: 0.7405, accuracy: 0.8404\n",
      "test loss: 0.6570, accuracy: 0.8573\n",
      "epoch: 5\n",
      "train loss: 0.6355, accuracy: 0.8533\n",
      "test loss: 0.5767, accuracy: 0.8692\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True)\n",
    "test_set = dezero.datasets.MNIST(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 10))\n",
    "optimizer = optimizers.SGD().setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)\n",
    "    ))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "    \n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "class ReLU(Function):\n",
    "    def forward(self, x):\n",
    "        y = np.maximum(x, 0.0)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        mask = x.data > 0\n",
    "        gx = gy * mask\n",
    "        return gx\n",
    "    \n",
    "def relu(x):\n",
    "    return ReLU()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP((hidden_size, hidden_size, 10), activation=F.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 1.0708, accuracy: 0.7749\n",
      "test loss: 0.4797, accuracy: 0.8872\n",
      "epoch: 2\n",
      "train loss: 0.4185, accuracy: 0.8900\n",
      "test loss: 0.3455, accuracy: 0.9081\n",
      "epoch: 3\n",
      "train loss: 0.3383, accuracy: 0.9063\n",
      "test loss: 0.3013, accuracy: 0.9172\n",
      "epoch: 4\n",
      "train loss: 0.3014, accuracy: 0.9155\n",
      "test loss: 0.2753, accuracy: 0.9244\n",
      "epoch: 5\n",
      "train loss: 0.2770, accuracy: 0.9222\n",
      "test loss: 0.2562, accuracy: 0.9297\n"
     ]
    }
   ],
   "source": [
    "# 학습 (ReLU 사용)\n",
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "\n",
    "optimizer = optimizers.SGD().setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)\n",
    "    ))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t) \n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "    \n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79e12847f48820b872cc8b14b94f40a7d044786e6f0c7be066ea50d15d55bebf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
